{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IN': 0, 'DT': 1, 'NNP': 2, 'CD': 3, 'NN': 4, '``': 5, \"''\": 6, 'POS': 7, '-LRB-': 8, 'VBN': 9, 'NNS': 10, 'VBP': 11, ',': 12, 'CC': 13, '-RRB-': 14, 'VBD': 15, 'RB': 16, 'TO': 17, '.': 18, 'VBZ': 19, 'NNPS': 20, 'PRP': 21, 'PRP$': 22, 'VB': 23, 'JJ': 24, 'MD': 25, 'VBG': 26, 'RBR': 27, ':': 28, 'WP': 29, 'WDT': 30, 'JJR': 31, 'PDT': 32, 'RBS': 33, 'WRB': 34, 'JJS': 35, '$': 36, 'RP': 37, 'FW': 38, 'EX': 39, 'SYM': 40, '#': 41, 'LS': 42, 'UH': 43, 'WP$': 44}\n",
      "{0: 'IN', 1: 'DT', 2: 'NNP', 3: 'CD', 4: 'NN', 5: '``', 6: \"''\", 7: 'POS', 8: '-LRB-', 9: 'VBN', 10: 'NNS', 11: 'VBP', 12: ',', 13: 'CC', 14: '-RRB-', 15: 'VBD', 16: 'RB', 17: 'TO', 18: '.', 19: 'VBZ', 20: 'NNPS', 21: 'PRP', 22: 'PRP$', 23: 'VB', 24: 'JJ', 25: 'MD', 26: 'VBG', 27: 'RBR', 28: ':', 29: 'WP', 30: 'WDT', 31: 'JJR', 32: 'PDT', 33: 'RBS', 34: 'WRB', 35: 'JJS', 36: '$', 37: 'RP', 38: 'FW', 39: 'EX', 40: 'SYM', 41: '#', 42: 'LS', 43: 'UH', 44: 'WP$'}\n",
      "{'i': 0, 'n': 1, 'a': 2, 'o': 3, 'c': 4, 't': 5, '.': 6, '1': 7, '9': 8, 'r': 9, 'e': 10, 'v': 11, 'w': 12, 'f': 13, '`': 14, 'h': 15, 'm': 16, 's': 17, 'p': 18, \"'\": 19, 'g': 20, 'd': 21, '(': 22, 'l': 23, 'z': 24, 'k': 25, 'y': 26, ',': 27, 'u': 28, '&': 29, ')': 30, 'b': 31, '-': 32, 'x': 33, '2': 34, '0': 35, '4': 36, 'q': 37, '5': 38, '8': 39, '7': 40, '%': 41, '{': 42, '}': 43, 'j': 44, '/': 45, '$': 46, '3': 47, ':': 48, '?': 49, ';': 50, '6': 51, '#': 52, '!': 53, '\\\\': 54, '*': 55, '=': 56, '@': 57, '\\r': 58}\n"
     ]
    }
   ],
   "source": [
    "# building additional dictionaries\n",
    "\n",
    "tag_idx = 0\n",
    "word_idx = 0\n",
    "char_idx = 0\n",
    "\n",
    "word2idx = dict()\n",
    "tag2idx = dict()\n",
    "idx2tag = dict()\n",
    "char2idx = dict()\n",
    "\n",
    "\n",
    "st = time.time()\n",
    "with open(\"corpus.train\", \"r\") as file:\n",
    "    for line in file:\n",
    "        pairs = line.rstrip(\"\\n\").rstrip(\"\\r\").split(\" \")\n",
    "    \n",
    "        for pair in pairs:\n",
    "            spliting_pair =  pair.split(\"/\")\n",
    "            word = \"/\".join(spliting_pair[:len(spliting_pair)-1]).lower()\n",
    "            tag = spliting_pair[len(spliting_pair)-1]\n",
    "            \n",
    "            \n",
    "            if word not in word2idx.keys():\n",
    "                word2idx[word] = word_idx\n",
    "                word_idx+=1\n",
    "                \n",
    "            if tag not in tag2idx.keys():\n",
    "                tag2idx[tag] = tag_idx\n",
    "                tag_idx+=1\n",
    "            \n",
    "            for char in word:\n",
    "                if char not in char2idx.keys():\n",
    "                    char2idx[char] = char_idx\n",
    "                    char_idx+=1\n",
    "                    \n",
    "for key in tag2idx:\n",
    "    idx2tag[tag2idx[key]] = key\n",
    "\n",
    "#Unknown word index\n",
    "word2idx[\"<UNKNOWN>\"] = word_idx\n",
    "word_idx+=1\n",
    "\n",
    "#Unknown char index\n",
    "char2idx['\\r'] = char_idx\n",
    "char_idx+=1\n",
    "\n",
    "\n",
    "print(tag2idx)\n",
    "print(idx2tag)\n",
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, filename):        \n",
    "        X, y = [], []\n",
    "        with open(filename, \"r\") as file:\n",
    "           \n",
    "            for line in file:\n",
    "                pairs = line.rstrip(\"\\n\").rstrip(\"\\r\").split(\" \")\n",
    "                \n",
    "                X1, y1 = [], []\n",
    "                \n",
    "                for pair in pairs:\n",
    "                    spliting_pair =  pair.split(\"/\")\n",
    "                    \n",
    "                    word = \"/\".join(spliting_pair[:len(spliting_pair)-1]).lower()\n",
    "                    tag = spliting_pair[len(spliting_pair)-1]\n",
    "                    \n",
    "                    X1.append(word)\n",
    "                    y1.append(tag)\n",
    "                \n",
    "                X.append(\" \".join(X1))\n",
    "                y.append(\" \".join(y1))\n",
    "                            \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "   \n",
    "    def __getitem__(self, idx):    \n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buildig neural network\n",
    "\n",
    "class POS_tag_net(nn.Module):\n",
    "    def __init__(self, vocab_size, char_size, target_size, \n",
    "                 word_emb_dim = 32, char_emb_dim = 8, \n",
    "                 out_channels = 16, kernel_size = 3,\n",
    "                 lstm_hidden_dim = 64):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.target_size = target_size\n",
    "        self.word_emb_dim = word_emb_dim\n",
    "        self.char_emb_dim = char_emb_dim\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        self.word_embedding = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self.char_embedding = nn.Embedding(char_size, char_emb_dim)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(self.char_emb_dim, out_channels=self.out_channels, kernel_size=self.kernel_size, padding=self.kernel_size//2)\n",
    "\n",
    "        self.lstm = nn.LSTM(self.word_emb_dim+self.out_channels, \n",
    "                           self.lstm_hidden_dim,\n",
    "                           bidirectional = True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.lstm_hidden_dim*2, self.target_size)\n",
    "    \n",
    "    def forward(self, words_batch, chars_batch, max_len_word, max_len_sentence):\n",
    "        \n",
    "        right_words_batch = []\n",
    "        for words in words_batch:\n",
    "            embedding = self.word_embedding(torch.tensor(words))\n",
    "            \n",
    "            if embedding.shape[0] != max_sentence_len:\n",
    "                embedding = torch.cat((embedding, torch.zeros(max_sentence_len-embedding.shape[0], self.word_emb_dim)))\n",
    "            \n",
    "            right_words_batch.append(embedding)\n",
    "        \n",
    "        right_words_batch = torch.stack(right_words_batch)\n",
    "        \n",
    "#         print(right_words_batch.shape)\n",
    "        \n",
    "        \n",
    "        right_chars_batch = []\n",
    "        \n",
    "        for words in chars_batch:\n",
    "            sentence = []\n",
    "            for chars in words:\n",
    "                c_embedding = self.char_embedding(torch.tensor(chars))\n",
    "            \n",
    "                if c_embedding.shape[0] != max_word_len:\n",
    "                    c_embedding = torch.cat((c_embedding, torch.zeros(max_word_len-c_embedding.shape[0], self.char_emb_dim)))\n",
    "                \n",
    "                sentence.append(torch.t(c_embedding))\n",
    "            \n",
    "            while len(sentence)!=max_len_sentence:\n",
    "                sentence.append(torch.t(torch.zeros(max_word_len, self.char_emb_dim)))\n",
    "                \n",
    "            sentence = torch.stack(sentence)\n",
    "            right_chars_batch.append(sentence)\n",
    "        \n",
    "        right_chars_batch = torch.stack(right_chars_batch)\n",
    "#         print(right_chars_batch.shape)\n",
    "        \n",
    "        right_chars_batch = right_chars_batch.view(-1, self.char_emb_dim, max_word_len)\n",
    "#         print(right_chars_batch.shape)\n",
    "        \n",
    "        right_chars_batch = F.relu(self.conv1(right_chars_batch))\n",
    "#         print(\"aaa\", right_chars_batch.shape)\n",
    "        \n",
    "        right_chars_batch, _ = torch.max(right_chars_batch, dim=-1)\n",
    "#         print(\"aaa\", right_chars_batch.shape)\n",
    "        \n",
    "        right_chars_batch = right_chars_batch.view(-1, max_len_sentence, self.out_channels)\n",
    "#         print(\"aaa\", right_chars_batch.shape)\n",
    "        \n",
    "        lstm_input = torch.cat((right_words_batch, right_chars_batch), dim=-1)\n",
    "        \n",
    "        lstm_output, _ = self.lstm(lstm_input)\n",
    "        \n",
    "        \n",
    "        out = F.log_softmax(self.fc1(lstm_output), dim = -1)\n",
    "#         print(out.shape)\n",
    "        return out\n",
    "\n",
    "    def loss(self, Y_hat, Y):\n",
    "        # TRICK 3 ********************************\n",
    "        # before we calculate the negative log likelihood, we need to mask out the activations\n",
    "        # this means we don't want to take into account padded items in the output vector\n",
    "        # simplest way to think about this is to flatten ALL sequences into a REALLY long sequence\n",
    "        # and calculate the loss on that.\n",
    "\n",
    "        # flatten all the labels\n",
    "        Y = Y.view(-1)\n",
    "\n",
    "        Y_hat = Y_hat.view(-1, 45)\n",
    "\n",
    "        mask = (Y > -1).float()\n",
    "\n",
    "        # count how many tokens we have\n",
    "        nb_tokens = int(torch.sum(mask).item())\n",
    "\n",
    "        # pick the values for the label and zero out the rest with the mask\n",
    "        Y_hat = Y_hat[range(Y_hat.shape[0]), Y] * mask\n",
    "\n",
    "        # compute cross entropy loss which ignores all <PAD> tokens\n",
    "        ce_loss = -torch.sum(Y_hat) / nb_tokens\n",
    "\n",
    "        return ce_loss\n",
    "        \n",
    "    \n",
    "my_net = POS_tag_net(len(word2idx.keys()), len(char2idx.keys()), len(tag2idx.keys()))\n",
    "\n",
    "optimezer = optim.Adam(my_net.parameters(), lr = 0.001)                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 1/1186 loss: 3.820404529571533\n",
      "Epoch: 0 Batch: 2/1186 loss: 3.78729510307312\n",
      "Epoch: 0 Batch: 3/1186 loss: 3.7655272483825684\n",
      "Epoch: 0 Batch: 4/1186 loss: 3.7398767471313477\n",
      "Epoch: 0 Batch: 5/1186 loss: 3.7185237407684326\n",
      "Epoch: 0 Batch: 6/1186 loss: 3.7019710540771484\n",
      "Epoch: 0 Batch: 7/1186 loss: 3.6738228797912598\n",
      "Epoch: 0 Batch: 8/1186 loss: 3.679391622543335\n",
      "Epoch: 0 Batch: 9/1186 loss: 3.623492956161499\n",
      "Epoch: 0 Batch: 10/1186 loss: 3.637249708175659\n",
      "Epoch: 0 Batch: 11/1186 loss: 3.579315423965454\n",
      "Epoch: 0 Batch: 12/1186 loss: 3.540592908859253\n",
      "Epoch: 0 Batch: 13/1186 loss: 3.479952573776245\n",
      "Epoch: 0 Batch: 14/1186 loss: 3.453953266143799\n",
      "Epoch: 0 Batch: 15/1186 loss: 3.3783583641052246\n",
      "Epoch: 0 Batch: 16/1186 loss: 3.3557252883911133\n",
      "Epoch: 0 Batch: 17/1186 loss: 3.4183707237243652\n",
      "Epoch: 0 Batch: 18/1186 loss: 3.3131446838378906\n",
      "Epoch: 0 Batch: 19/1186 loss: 3.248081922531128\n",
      "Epoch: 0 Batch: 20/1186 loss: 3.1276755332946777\n",
      "Epoch: 0 Batch: 21/1186 loss: 3.0685312747955322\n",
      "Epoch: 0 Batch: 22/1186 loss: 3.050440788269043\n",
      "Epoch: 0 Batch: 23/1186 loss: 2.9518349170684814\n",
      "Epoch: 0 Batch: 24/1186 loss: 2.967456817626953\n",
      "Epoch: 0 Batch: 25/1186 loss: 2.9887189865112305\n",
      "Epoch: 0 Batch: 26/1186 loss: 2.8720815181732178\n",
      "Epoch: 0 Batch: 27/1186 loss: 2.967043161392212\n",
      "Epoch: 0 Batch: 28/1186 loss: 2.8539865016937256\n",
      "Epoch: 0 Batch: 29/1186 loss: 2.841911554336548\n",
      "Epoch: 0 Batch: 30/1186 loss: 2.8931753635406494\n",
      "Epoch: 0 Batch: 31/1186 loss: 2.880474090576172\n",
      "Epoch: 0 Batch: 32/1186 loss: 2.845658779144287\n",
      "Epoch: 0 Batch: 33/1186 loss: 2.7105486392974854\n",
      "Epoch: 0 Batch: 34/1186 loss: 2.8249142169952393\n",
      "Epoch: 0 Batch: 35/1186 loss: 2.9121038913726807\n",
      "Epoch: 0 Batch: 36/1186 loss: 2.8097331523895264\n",
      "Epoch: 0 Batch: 37/1186 loss: 2.6469292640686035\n",
      "Epoch: 0 Batch: 38/1186 loss: 2.7102370262145996\n",
      "Epoch: 0 Batch: 39/1186 loss: 2.7047863006591797\n",
      "Epoch: 0 Batch: 40/1186 loss: 2.7509500980377197\n",
      "Epoch: 0 Batch: 41/1186 loss: 2.6920478343963623\n",
      "Epoch: 0 Batch: 42/1186 loss: 2.6588621139526367\n",
      "Epoch: 0 Batch: 43/1186 loss: 2.6512482166290283\n",
      "Epoch: 0 Batch: 44/1186 loss: 2.5896780490875244\n",
      "Epoch: 0 Batch: 45/1186 loss: 2.618103265762329\n",
      "Epoch: 0 Batch: 46/1186 loss: 2.4775161743164062\n",
      "Epoch: 0 Batch: 47/1186 loss: 2.610358238220215\n",
      "Epoch: 0 Batch: 48/1186 loss: 2.643597364425659\n",
      "Epoch: 0 Batch: 49/1186 loss: 2.634316921234131\n",
      "Epoch: 0 Batch: 50/1186 loss: 2.5641393661499023\n",
      "Epoch: 0 Batch: 51/1186 loss: 2.5600547790527344\n",
      "Epoch: 0 Batch: 52/1186 loss: 2.521969795227051\n",
      "Epoch: 0 Batch: 53/1186 loss: 2.5102570056915283\n",
      "Epoch: 0 Batch: 54/1186 loss: 2.4966869354248047\n",
      "Epoch: 0 Batch: 55/1186 loss: 2.473607063293457\n",
      "Epoch: 0 Batch: 56/1186 loss: 2.392883062362671\n",
      "Epoch: 0 Batch: 57/1186 loss: 2.621321201324463\n",
      "Epoch: 0 Batch: 58/1186 loss: 2.4661641120910645\n",
      "Epoch: 0 Batch: 59/1186 loss: 2.6232540607452393\n",
      "Epoch: 0 Batch: 60/1186 loss: 2.8481314182281494\n",
      "Epoch: 0 Batch: 61/1186 loss: 2.412564754486084\n",
      "Epoch: 0 Batch: 62/1186 loss: 2.320460319519043\n",
      "Epoch: 0 Batch: 63/1186 loss: 2.366380453109741\n",
      "Epoch: 0 Batch: 64/1186 loss: 2.3263986110687256\n",
      "Epoch: 0 Batch: 65/1186 loss: 2.2300453186035156\n",
      "Epoch: 0 Batch: 66/1186 loss: 2.2522637844085693\n",
      "Epoch: 0 Batch: 67/1186 loss: 2.4102087020874023\n",
      "Epoch: 0 Batch: 68/1186 loss: 2.395388603210449\n",
      "Epoch: 0 Batch: 69/1186 loss: 2.270402669906616\n",
      "Epoch: 0 Batch: 70/1186 loss: 2.285287857055664\n",
      "Epoch: 0 Batch: 71/1186 loss: 2.3310844898223877\n",
      "Epoch: 0 Batch: 72/1186 loss: 2.216580390930176\n",
      "Epoch: 0 Batch: 73/1186 loss: 2.2147955894470215\n",
      "Epoch: 0 Batch: 74/1186 loss: 2.2524540424346924\n",
      "Epoch: 0 Batch: 75/1186 loss: 2.1375575065612793\n",
      "Epoch: 0 Batch: 76/1186 loss: 2.027259588241577\n",
      "Epoch: 0 Batch: 77/1186 loss: 2.0461809635162354\n",
      "Epoch: 0 Batch: 78/1186 loss: 2.195308208465576\n",
      "Epoch: 0 Batch: 79/1186 loss: 2.2515549659729004\n",
      "Epoch: 0 Batch: 80/1186 loss: 2.165827512741089\n",
      "Epoch: 0 Batch: 81/1186 loss: 2.056979179382324\n",
      "Epoch: 0 Batch: 82/1186 loss: 2.0495142936706543\n",
      "Epoch: 0 Batch: 83/1186 loss: 2.199779987335205\n",
      "Epoch: 0 Batch: 84/1186 loss: 2.0986273288726807\n",
      "Epoch: 0 Batch: 85/1186 loss: 2.162151336669922\n",
      "Epoch: 0 Batch: 86/1186 loss: 2.1333470344543457\n",
      "Epoch: 0 Batch: 87/1186 loss: 1.973610758781433\n",
      "Epoch: 0 Batch: 88/1186 loss: 2.0277602672576904\n",
      "Epoch: 0 Batch: 89/1186 loss: 1.920199990272522\n",
      "Epoch: 0 Batch: 90/1186 loss: 1.96603524684906\n",
      "Epoch: 0 Batch: 91/1186 loss: 1.8935794830322266\n",
      "Epoch: 0 Batch: 92/1186 loss: 2.0496106147766113\n",
      "Epoch: 0 Batch: 93/1186 loss: 2.1896626949310303\n",
      "Epoch: 0 Batch: 94/1186 loss: 1.9785408973693848\n",
      "Epoch: 0 Batch: 95/1186 loss: 1.8946770429611206\n",
      "Epoch: 0 Batch: 96/1186 loss: 1.9683934450149536\n",
      "Epoch: 0 Batch: 97/1186 loss: 1.9715301990509033\n",
      "Epoch: 0 Batch: 98/1186 loss: 1.7657065391540527\n",
      "Epoch: 0 Batch: 99/1186 loss: 2.021364450454712\n",
      "Epoch: 0 Batch: 100/1186 loss: 2.011953592300415\n",
      "Epoch: 0 Batch: 101/1186 loss: 1.992451548576355\n",
      "Epoch: 0 Batch: 102/1186 loss: 1.8767296075820923\n",
      "Epoch: 0 Batch: 103/1186 loss: 1.9434477090835571\n",
      "Epoch: 0 Batch: 104/1186 loss: 2.0398244857788086\n",
      "Epoch: 0 Batch: 105/1186 loss: 1.7725101709365845\n",
      "Epoch: 0 Batch: 106/1186 loss: 2.0012903213500977\n",
      "Epoch: 0 Batch: 107/1186 loss: 1.807555913925171\n",
      "Epoch: 0 Batch: 108/1186 loss: 1.9085451364517212\n",
      "Epoch: 0 Batch: 109/1186 loss: 1.841969609260559\n",
      "Epoch: 0 Batch: 110/1186 loss: 1.7653677463531494\n",
      "Epoch: 0 Batch: 111/1186 loss: 1.8647139072418213\n",
      "Epoch: 0 Batch: 112/1186 loss: 1.8319997787475586\n",
      "Epoch: 0 Batch: 113/1186 loss: 1.724063754081726\n",
      "Epoch: 0 Batch: 114/1186 loss: 1.8455467224121094\n",
      "Epoch: 0 Batch: 115/1186 loss: 1.811413288116455\n",
      "Epoch: 0 Batch: 116/1186 loss: 1.7913157939910889\n",
      "Epoch: 0 Batch: 117/1186 loss: 1.9781825542449951\n",
      "Epoch: 0 Batch: 118/1186 loss: 2.0124340057373047\n",
      "Epoch: 0 Batch: 119/1186 loss: 1.8784749507904053\n",
      "Epoch: 0 Batch: 120/1186 loss: 1.965100884437561\n",
      "Epoch: 0 Batch: 121/1186 loss: 1.8297053575515747\n",
      "Epoch: 0 Batch: 122/1186 loss: 1.9005000591278076\n",
      "Epoch: 0 Batch: 123/1186 loss: 1.9633392095565796\n",
      "Epoch: 0 Batch: 124/1186 loss: 1.825320839881897\n",
      "Epoch: 0 Batch: 125/1186 loss: 1.8299986124038696\n",
      "Epoch: 0 Batch: 126/1186 loss: 1.7599762678146362\n",
      "Epoch: 0 Batch: 127/1186 loss: 1.7276281118392944\n",
      "Epoch: 0 Batch: 128/1186 loss: 1.8211311101913452\n",
      "Epoch: 0 Batch: 129/1186 loss: 1.8081024885177612\n",
      "Epoch: 0 Batch: 130/1186 loss: 1.7663265466690063\n",
      "Epoch: 0 Batch: 131/1186 loss: 1.7272354364395142\n",
      "Epoch: 0 Batch: 132/1186 loss: 1.7048923969268799\n",
      "Epoch: 0 Batch: 133/1186 loss: 1.7874066829681396\n",
      "Epoch: 0 Batch: 134/1186 loss: 1.5563756227493286\n",
      "Epoch: 0 Batch: 135/1186 loss: 1.7062053680419922\n",
      "Epoch: 0 Batch: 136/1186 loss: 1.676042079925537\n",
      "Epoch: 0 Batch: 137/1186 loss: 1.6879236698150635\n",
      "Epoch: 0 Batch: 138/1186 loss: 1.640476942062378\n",
      "Epoch: 0 Batch: 139/1186 loss: 1.5812994241714478\n",
      "Epoch: 0 Batch: 140/1186 loss: 1.7425974607467651\n",
      "Epoch: 0 Batch: 141/1186 loss: 1.6919862031936646\n",
      "Epoch: 0 Batch: 142/1186 loss: 1.67405104637146\n",
      "Epoch: 0 Batch: 143/1186 loss: 1.7080903053283691\n",
      "Epoch: 0 Batch: 144/1186 loss: 1.8041181564331055\n",
      "Epoch: 0 Batch: 145/1186 loss: 1.6554852724075317\n",
      "Epoch: 0 Batch: 146/1186 loss: 1.776350736618042\n",
      "Epoch: 0 Batch: 147/1186 loss: 1.6008838415145874\n",
      "Epoch: 0 Batch: 148/1186 loss: 1.5998555421829224\n",
      "Epoch: 0 Batch: 149/1186 loss: 1.615108609199524\n",
      "Epoch: 0 Batch: 150/1186 loss: 1.5213536024093628\n",
      "Epoch: 0 Batch: 151/1186 loss: 1.5513705015182495\n",
      "Epoch: 0 Batch: 152/1186 loss: 1.7344802618026733\n",
      "Epoch: 0 Batch: 153/1186 loss: 1.6422090530395508\n",
      "Epoch: 0 Batch: 154/1186 loss: 1.6279610395431519\n",
      "Epoch: 0 Batch: 155/1186 loss: 1.6985656023025513\n",
      "Epoch: 0 Batch: 156/1186 loss: 1.928818941116333\n",
      "Epoch: 0 Batch: 157/1186 loss: 1.636825680732727\n",
      "Epoch: 0 Batch: 158/1186 loss: 1.5851819515228271\n",
      "Epoch: 0 Batch: 159/1186 loss: 1.6554559469223022\n",
      "Epoch: 0 Batch: 160/1186 loss: 1.6248080730438232\n",
      "Epoch: 0 Batch: 161/1186 loss: 1.569103479385376\n",
      "Epoch: 0 Batch: 162/1186 loss: 1.679559350013733\n",
      "Epoch: 0 Batch: 163/1186 loss: 1.5992357730865479\n",
      "Epoch: 0 Batch: 164/1186 loss: 1.5316123962402344\n",
      "Epoch: 0 Batch: 165/1186 loss: 1.4687250852584839\n",
      "Epoch: 0 Batch: 166/1186 loss: 1.720801591873169\n",
      "Epoch: 0 Batch: 167/1186 loss: 1.563444972038269\n",
      "Epoch: 0 Batch: 168/1186 loss: 1.4240670204162598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 169/1186 loss: 1.5287188291549683\n",
      "Epoch: 0 Batch: 170/1186 loss: 1.5277125835418701\n",
      "Epoch: 0 Batch: 171/1186 loss: 1.5492793321609497\n",
      "Epoch: 0 Batch: 172/1186 loss: 1.6274393796920776\n",
      "Epoch: 0 Batch: 173/1186 loss: 1.457971215248108\n",
      "Epoch: 0 Batch: 174/1186 loss: 1.514896035194397\n",
      "Epoch: 0 Batch: 175/1186 loss: 1.4544639587402344\n",
      "Epoch: 0 Batch: 176/1186 loss: 1.5065146684646606\n",
      "Epoch: 0 Batch: 177/1186 loss: 1.4411065578460693\n",
      "Epoch: 0 Batch: 178/1186 loss: 1.4912947416305542\n",
      "Epoch: 0 Batch: 179/1186 loss: 1.3915094137191772\n",
      "Epoch: 0 Batch: 180/1186 loss: 1.5430957078933716\n",
      "Epoch: 0 Batch: 181/1186 loss: 1.448546290397644\n",
      "Epoch: 0 Batch: 182/1186 loss: 1.4029890298843384\n",
      "Epoch: 0 Batch: 183/1186 loss: 1.5664469003677368\n",
      "Epoch: 0 Batch: 184/1186 loss: 1.4482345581054688\n",
      "Epoch: 0 Batch: 185/1186 loss: 1.4787853956222534\n",
      "Epoch: 0 Batch: 186/1186 loss: 1.4324078559875488\n",
      "Epoch: 0 Batch: 187/1186 loss: 1.6121609210968018\n",
      "Epoch: 0 Batch: 188/1186 loss: 1.4083739519119263\n",
      "Epoch: 0 Batch: 189/1186 loss: 1.5508257150650024\n",
      "Epoch: 0 Batch: 190/1186 loss: 1.534826397895813\n",
      "Epoch: 0 Batch: 191/1186 loss: 1.619836449623108\n",
      "Epoch: 0 Batch: 192/1186 loss: 1.5579969882965088\n",
      "Epoch: 0 Batch: 193/1186 loss: 1.6261420249938965\n",
      "Epoch: 0 Batch: 194/1186 loss: 1.4189647436141968\n",
      "Epoch: 0 Batch: 195/1186 loss: 1.5007357597351074\n",
      "Epoch: 0 Batch: 196/1186 loss: 1.6744911670684814\n",
      "Epoch: 0 Batch: 197/1186 loss: 1.4630868434906006\n",
      "Epoch: 0 Batch: 198/1186 loss: 1.412508249282837\n",
      "Epoch: 0 Batch: 199/1186 loss: 1.489601492881775\n",
      "Epoch: 0 Batch: 200/1186 loss: 1.4434449672698975\n",
      "Epoch: 0 Batch: 201/1186 loss: 1.5181102752685547\n",
      "Epoch: 0 Batch: 202/1186 loss: 1.6167082786560059\n",
      "Epoch: 0 Batch: 203/1186 loss: 1.4520264863967896\n",
      "Epoch: 0 Batch: 204/1186 loss: 1.3863632678985596\n",
      "Epoch: 0 Batch: 205/1186 loss: 1.4346821308135986\n",
      "Epoch: 0 Batch: 206/1186 loss: 1.439457893371582\n",
      "Epoch: 0 Batch: 207/1186 loss: 1.3457388877868652\n",
      "Epoch: 0 Batch: 208/1186 loss: 1.361072063446045\n",
      "Epoch: 0 Batch: 209/1186 loss: 1.4012562036514282\n",
      "Epoch: 0 Batch: 210/1186 loss: 1.4435622692108154\n",
      "Epoch: 0 Batch: 211/1186 loss: 1.4988270998001099\n",
      "Epoch: 0 Batch: 212/1186 loss: 1.5731266736984253\n",
      "Epoch: 0 Batch: 213/1186 loss: 1.5313677787780762\n",
      "Epoch: 0 Batch: 214/1186 loss: 1.395639181137085\n",
      "Epoch: 0 Batch: 215/1186 loss: 1.4080774784088135\n",
      "Epoch: 0 Batch: 216/1186 loss: 1.4195234775543213\n",
      "Epoch: 0 Batch: 217/1186 loss: 1.3711344003677368\n",
      "Epoch: 0 Batch: 218/1186 loss: 1.356669545173645\n",
      "Epoch: 0 Batch: 219/1186 loss: 1.3985435962677002\n",
      "Epoch: 0 Batch: 220/1186 loss: 1.351367712020874\n",
      "Epoch: 0 Batch: 221/1186 loss: 1.3531159162521362\n",
      "Epoch: 0 Batch: 222/1186 loss: 1.3145807981491089\n",
      "Epoch: 0 Batch: 223/1186 loss: 1.3638750314712524\n",
      "Epoch: 0 Batch: 224/1186 loss: 1.392444133758545\n",
      "Epoch: 0 Batch: 225/1186 loss: 1.4210511445999146\n",
      "Epoch: 0 Batch: 226/1186 loss: 1.3659067153930664\n",
      "Epoch: 0 Batch: 227/1186 loss: 1.302334189414978\n",
      "Epoch: 0 Batch: 228/1186 loss: 1.344157099723816\n",
      "Epoch: 0 Batch: 229/1186 loss: 1.3458691835403442\n",
      "Epoch: 0 Batch: 230/1186 loss: 1.4262479543685913\n",
      "Epoch: 0 Batch: 231/1186 loss: 1.3369632959365845\n",
      "Epoch: 0 Batch: 232/1186 loss: 1.3879746198654175\n",
      "Epoch: 0 Batch: 233/1186 loss: 1.3952256441116333\n",
      "Epoch: 0 Batch: 234/1186 loss: 1.363331913948059\n",
      "Epoch: 0 Batch: 235/1186 loss: 1.4383476972579956\n",
      "Epoch: 0 Batch: 236/1186 loss: 1.311780333518982\n",
      "Epoch: 0 Batch: 237/1186 loss: 1.480494737625122\n",
      "Epoch: 0 Batch: 238/1186 loss: 1.404160976409912\n",
      "Epoch: 0 Batch: 239/1186 loss: 1.3707962036132812\n",
      "Epoch: 0 Batch: 240/1186 loss: 1.288944959640503\n",
      "Epoch: 0 Batch: 241/1186 loss: 1.3896218538284302\n",
      "Epoch: 0 Batch: 242/1186 loss: 1.508880376815796\n",
      "Epoch: 0 Batch: 243/1186 loss: 1.4609317779541016\n",
      "Epoch: 0 Batch: 244/1186 loss: 1.34238862991333\n",
      "Epoch: 0 Batch: 245/1186 loss: 1.2857205867767334\n",
      "Epoch: 0 Batch: 246/1186 loss: 1.2488646507263184\n",
      "Epoch: 0 Batch: 247/1186 loss: 1.3093183040618896\n",
      "Epoch: 0 Batch: 248/1186 loss: 1.376879334449768\n",
      "Epoch: 0 Batch: 249/1186 loss: 1.3756382465362549\n",
      "Epoch: 0 Batch: 250/1186 loss: 1.234171748161316\n",
      "Epoch: 0 Batch: 251/1186 loss: 1.2918895483016968\n",
      "Epoch: 0 Batch: 252/1186 loss: 1.418116807937622\n",
      "Epoch: 0 Batch: 253/1186 loss: 1.4005835056304932\n",
      "Epoch: 0 Batch: 254/1186 loss: 1.2548731565475464\n",
      "Epoch: 0 Batch: 255/1186 loss: 1.307415246963501\n",
      "Epoch: 0 Batch: 256/1186 loss: 1.2999210357666016\n",
      "Epoch: 0 Batch: 257/1186 loss: 1.3091163635253906\n",
      "Epoch: 0 Batch: 258/1186 loss: 1.4150607585906982\n",
      "Epoch: 0 Batch: 259/1186 loss: 1.189614176750183\n",
      "Epoch: 0 Batch: 260/1186 loss: 1.1775072813034058\n",
      "Epoch: 0 Batch: 261/1186 loss: 1.252943754196167\n",
      "Epoch: 0 Batch: 262/1186 loss: 1.2650353908538818\n",
      "Epoch: 0 Batch: 263/1186 loss: 1.2026766538619995\n",
      "Epoch: 0 Batch: 264/1186 loss: 1.245044231414795\n",
      "Epoch: 0 Batch: 265/1186 loss: 1.2742702960968018\n",
      "Epoch: 0 Batch: 266/1186 loss: 1.3375163078308105\n",
      "Epoch: 0 Batch: 267/1186 loss: 1.278182029724121\n",
      "Epoch: 0 Batch: 268/1186 loss: 1.2840766906738281\n",
      "Epoch: 0 Batch: 269/1186 loss: 1.1971739530563354\n",
      "Epoch: 0 Batch: 270/1186 loss: 1.2330536842346191\n",
      "Epoch: 0 Batch: 271/1186 loss: 1.197571039199829\n",
      "Epoch: 0 Batch: 272/1186 loss: 1.2532087564468384\n",
      "Epoch: 0 Batch: 273/1186 loss: 1.230114221572876\n",
      "Epoch: 0 Batch: 274/1186 loss: 1.2613003253936768\n",
      "Epoch: 0 Batch: 275/1186 loss: 1.1821337938308716\n",
      "Epoch: 0 Batch: 276/1186 loss: 1.3640304803848267\n",
      "Epoch: 0 Batch: 277/1186 loss: 1.2327969074249268\n",
      "Epoch: 0 Batch: 278/1186 loss: 1.2051547765731812\n",
      "Epoch: 0 Batch: 279/1186 loss: 1.2469018697738647\n",
      "Epoch: 0 Batch: 280/1186 loss: 1.1235897541046143\n",
      "Epoch: 0 Batch: 281/1186 loss: 1.1479569673538208\n",
      "Epoch: 0 Batch: 282/1186 loss: 1.169382095336914\n",
      "Epoch: 0 Batch: 283/1186 loss: 1.1255052089691162\n",
      "Epoch: 0 Batch: 284/1186 loss: 1.191322922706604\n",
      "Epoch: 0 Batch: 285/1186 loss: 1.1927286386489868\n",
      "Epoch: 0 Batch: 286/1186 loss: 1.2098160982131958\n",
      "Epoch: 0 Batch: 287/1186 loss: 1.199552059173584\n",
      "Epoch: 0 Batch: 288/1186 loss: 1.1731586456298828\n",
      "Epoch: 0 Batch: 289/1186 loss: 1.2198444604873657\n",
      "Epoch: 0 Batch: 290/1186 loss: 1.3413305282592773\n",
      "Epoch: 0 Batch: 291/1186 loss: 1.2604279518127441\n",
      "Epoch: 0 Batch: 292/1186 loss: 1.1982100009918213\n",
      "Epoch: 0 Batch: 293/1186 loss: 1.2141464948654175\n",
      "Epoch: 0 Batch: 294/1186 loss: 1.2055846452713013\n",
      "Epoch: 0 Batch: 295/1186 loss: 1.36865234375\n",
      "Epoch: 0 Batch: 296/1186 loss: 1.1432690620422363\n",
      "Epoch: 0 Batch: 297/1186 loss: 1.2163612842559814\n",
      "Epoch: 0 Batch: 298/1186 loss: 1.2137335538864136\n",
      "Epoch: 0 Batch: 299/1186 loss: 1.2369779348373413\n",
      "Epoch: 0 Batch: 300/1186 loss: 1.2044119834899902\n",
      "Epoch: 0 Batch: 301/1186 loss: 1.2396968603134155\n",
      "Epoch: 0 Batch: 302/1186 loss: 1.1545288562774658\n",
      "Epoch: 0 Batch: 303/1186 loss: 1.0884931087493896\n",
      "Epoch: 0 Batch: 304/1186 loss: 1.1043809652328491\n",
      "Epoch: 0 Batch: 305/1186 loss: 1.324657917022705\n",
      "Epoch: 0 Batch: 306/1186 loss: 1.0879002809524536\n",
      "Epoch: 0 Batch: 307/1186 loss: 1.2567895650863647\n",
      "Epoch: 0 Batch: 308/1186 loss: 1.1440435647964478\n",
      "Epoch: 0 Batch: 309/1186 loss: 1.11538827419281\n",
      "Epoch: 0 Batch: 310/1186 loss: 1.149463176727295\n",
      "Epoch: 0 Batch: 311/1186 loss: 1.1832770109176636\n",
      "Epoch: 0 Batch: 312/1186 loss: 1.0916136503219604\n",
      "Epoch: 0 Batch: 313/1186 loss: 1.2280793190002441\n",
      "Epoch: 0 Batch: 314/1186 loss: 1.158622145652771\n",
      "Epoch: 0 Batch: 315/1186 loss: 1.1037708520889282\n",
      "Epoch: 0 Batch: 316/1186 loss: 1.2126342058181763\n",
      "Epoch: 0 Batch: 317/1186 loss: 1.3407567739486694\n",
      "Epoch: 0 Batch: 318/1186 loss: 1.1289361715316772\n",
      "Epoch: 0 Batch: 319/1186 loss: 1.1278482675552368\n",
      "Epoch: 0 Batch: 320/1186 loss: 1.0461927652359009\n",
      "Epoch: 0 Batch: 321/1186 loss: 1.1610450744628906\n",
      "Epoch: 0 Batch: 322/1186 loss: 1.1589034795761108\n",
      "Epoch: 0 Batch: 323/1186 loss: 1.097928762435913\n",
      "Epoch: 0 Batch: 324/1186 loss: 1.0783448219299316\n",
      "Epoch: 0 Batch: 325/1186 loss: 1.1165225505828857\n",
      "Epoch: 0 Batch: 326/1186 loss: 1.2469151020050049\n",
      "Epoch: 0 Batch: 327/1186 loss: 1.1771442890167236\n",
      "Epoch: 0 Batch: 328/1186 loss: 1.2409003973007202\n",
      "Epoch: 0 Batch: 329/1186 loss: 1.1656863689422607\n",
      "Epoch: 0 Batch: 330/1186 loss: 1.1574465036392212\n",
      "Epoch: 0 Batch: 331/1186 loss: 1.103716254234314\n",
      "Epoch: 0 Batch: 332/1186 loss: 1.1328104734420776\n",
      "Epoch: 0 Batch: 333/1186 loss: 1.1694940328598022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 334/1186 loss: 1.2469837665557861\n",
      "Epoch: 0 Batch: 335/1186 loss: 1.2812048196792603\n",
      "Epoch: 0 Batch: 336/1186 loss: 1.0647838115692139\n",
      "Epoch: 0 Batch: 337/1186 loss: 1.0551412105560303\n",
      "Epoch: 0 Batch: 338/1186 loss: 1.0435854196548462\n",
      "Epoch: 0 Batch: 339/1186 loss: 1.1028565168380737\n",
      "Epoch: 0 Batch: 340/1186 loss: 1.1234705448150635\n",
      "Epoch: 0 Batch: 341/1186 loss: 1.1224489212036133\n",
      "Epoch: 0 Batch: 342/1186 loss: 1.1543594598770142\n",
      "Epoch: 0 Batch: 343/1186 loss: 1.1589972972869873\n",
      "Epoch: 0 Batch: 344/1186 loss: 1.2261772155761719\n",
      "Epoch: 0 Batch: 345/1186 loss: 1.2052335739135742\n",
      "Epoch: 0 Batch: 346/1186 loss: 1.217176079750061\n",
      "Epoch: 0 Batch: 347/1186 loss: 1.0590736865997314\n",
      "Epoch: 0 Batch: 348/1186 loss: 1.1133091449737549\n",
      "Epoch: 0 Batch: 349/1186 loss: 1.1278855800628662\n",
      "Epoch: 0 Batch: 350/1186 loss: 1.1397182941436768\n",
      "Epoch: 0 Batch: 351/1186 loss: 1.1889607906341553\n",
      "Epoch: 0 Batch: 352/1186 loss: 1.1261130571365356\n",
      "Epoch: 0 Batch: 353/1186 loss: 1.1087652444839478\n",
      "Epoch: 0 Batch: 354/1186 loss: 1.0631139278411865\n",
      "Epoch: 0 Batch: 355/1186 loss: 1.1659588813781738\n",
      "Epoch: 0 Batch: 356/1186 loss: 1.0698761940002441\n",
      "Epoch: 0 Batch: 357/1186 loss: 1.012068748474121\n",
      "Epoch: 0 Batch: 358/1186 loss: 1.0207610130310059\n",
      "Epoch: 0 Batch: 359/1186 loss: 0.9702439308166504\n",
      "Epoch: 0 Batch: 360/1186 loss: 1.0794342756271362\n",
      "Epoch: 0 Batch: 361/1186 loss: 1.0209928750991821\n",
      "Epoch: 0 Batch: 362/1186 loss: 1.034760594367981\n",
      "Epoch: 0 Batch: 363/1186 loss: 1.0207916498184204\n",
      "Epoch: 0 Batch: 364/1186 loss: 1.0711724758148193\n",
      "Epoch: 0 Batch: 365/1186 loss: 1.0820008516311646\n",
      "Epoch: 0 Batch: 366/1186 loss: 0.9703957438468933\n",
      "Epoch: 0 Batch: 367/1186 loss: 1.0954909324645996\n",
      "Epoch: 0 Batch: 368/1186 loss: 1.0415358543395996\n",
      "Epoch: 0 Batch: 369/1186 loss: 1.02176833152771\n",
      "Epoch: 0 Batch: 370/1186 loss: 1.0250201225280762\n",
      "Epoch: 0 Batch: 371/1186 loss: 1.0280354022979736\n",
      "Epoch: 0 Batch: 372/1186 loss: 1.1077066659927368\n",
      "Epoch: 0 Batch: 373/1186 loss: 1.0084115266799927\n",
      "Epoch: 0 Batch: 374/1186 loss: 1.0544519424438477\n",
      "Epoch: 0 Batch: 375/1186 loss: 0.9843406081199646\n",
      "Epoch: 0 Batch: 376/1186 loss: 1.0487589836120605\n",
      "Epoch: 0 Batch: 377/1186 loss: 1.0801455974578857\n",
      "Epoch: 0 Batch: 378/1186 loss: 1.0166951417922974\n",
      "Epoch: 0 Batch: 379/1186 loss: 1.0684809684753418\n",
      "Epoch: 0 Batch: 380/1186 loss: 1.0625849962234497\n",
      "Epoch: 0 Batch: 381/1186 loss: 1.0747748613357544\n",
      "Epoch: 0 Batch: 382/1186 loss: 1.059228777885437\n",
      "Epoch: 0 Batch: 383/1186 loss: 0.9550914764404297\n",
      "Epoch: 0 Batch: 384/1186 loss: 0.9894053339958191\n",
      "Epoch: 0 Batch: 385/1186 loss: 1.0090690851211548\n",
      "Epoch: 0 Batch: 386/1186 loss: 1.0764089822769165\n",
      "Epoch: 0 Batch: 387/1186 loss: 0.8903061747550964\n",
      "Epoch: 0 Batch: 388/1186 loss: 1.0078140497207642\n",
      "Epoch: 0 Batch: 389/1186 loss: 1.2307125329971313\n",
      "Epoch: 0 Batch: 390/1186 loss: 1.1107981204986572\n",
      "Epoch: 0 Batch: 391/1186 loss: 1.0344685316085815\n",
      "Epoch: 0 Batch: 392/1186 loss: 0.9568576812744141\n",
      "Epoch: 0 Batch: 393/1186 loss: 0.9306814670562744\n",
      "Epoch: 0 Batch: 394/1186 loss: 1.0349465608596802\n",
      "Epoch: 0 Batch: 395/1186 loss: 1.0551286935806274\n",
      "Epoch: 0 Batch: 396/1186 loss: 0.882305920124054\n",
      "Epoch: 0 Batch: 397/1186 loss: 0.9319837689399719\n",
      "Epoch: 0 Batch: 398/1186 loss: 1.0863301753997803\n",
      "Epoch: 0 Batch: 399/1186 loss: 1.0797743797302246\n",
      "Epoch: 0 Batch: 400/1186 loss: 1.0191051959991455\n",
      "Epoch: 0 Batch: 401/1186 loss: 1.0313856601715088\n",
      "Epoch: 0 Batch: 402/1186 loss: 1.0143940448760986\n",
      "Epoch: 0 Batch: 403/1186 loss: 1.0521643161773682\n",
      "Epoch: 0 Batch: 404/1186 loss: 0.9619562029838562\n",
      "Epoch: 0 Batch: 405/1186 loss: 0.9351937770843506\n",
      "Epoch: 0 Batch: 406/1186 loss: 0.9545290470123291\n",
      "Epoch: 0 Batch: 407/1186 loss: 1.058069109916687\n",
      "Epoch: 0 Batch: 408/1186 loss: 1.0855404138565063\n",
      "Epoch: 0 Batch: 409/1186 loss: 1.011417031288147\n",
      "Epoch: 0 Batch: 410/1186 loss: 0.9609168171882629\n",
      "Epoch: 0 Batch: 411/1186 loss: 0.9397828578948975\n",
      "Epoch: 0 Batch: 412/1186 loss: 0.997615396976471\n",
      "Epoch: 0 Batch: 413/1186 loss: 0.9527857899665833\n",
      "Epoch: 0 Batch: 414/1186 loss: 1.072980523109436\n",
      "Epoch: 0 Batch: 415/1186 loss: 1.0130550861358643\n",
      "Epoch: 0 Batch: 416/1186 loss: 1.0300710201263428\n",
      "Epoch: 0 Batch: 417/1186 loss: 1.0335177183151245\n",
      "Epoch: 0 Batch: 418/1186 loss: 0.9769225120544434\n",
      "Epoch: 0 Batch: 419/1186 loss: 1.0956366062164307\n",
      "Epoch: 0 Batch: 420/1186 loss: 1.0065168142318726\n",
      "Epoch: 0 Batch: 421/1186 loss: 0.9312164187431335\n",
      "Epoch: 0 Batch: 422/1186 loss: 0.8258780241012573\n",
      "Epoch: 0 Batch: 423/1186 loss: 1.0753436088562012\n",
      "Epoch: 0 Batch: 424/1186 loss: 0.9723861217498779\n",
      "Epoch: 0 Batch: 425/1186 loss: 0.8744818568229675\n",
      "Epoch: 0 Batch: 426/1186 loss: 0.9474596977233887\n",
      "Epoch: 0 Batch: 427/1186 loss: 1.0686408281326294\n",
      "Epoch: 0 Batch: 428/1186 loss: 0.9643635749816895\n",
      "Epoch: 0 Batch: 429/1186 loss: 0.9658843278884888\n",
      "Epoch: 0 Batch: 430/1186 loss: 0.8996621370315552\n",
      "Epoch: 0 Batch: 431/1186 loss: 1.018186330795288\n",
      "Epoch: 0 Batch: 432/1186 loss: 1.016404151916504\n",
      "Epoch: 0 Batch: 433/1186 loss: 1.0075360536575317\n",
      "Epoch: 0 Batch: 434/1186 loss: 1.1876929998397827\n",
      "Epoch: 0 Batch: 435/1186 loss: 1.0806611776351929\n",
      "Epoch: 0 Batch: 436/1186 loss: 0.8080737590789795\n",
      "Epoch: 0 Batch: 437/1186 loss: 0.962948203086853\n",
      "Epoch: 0 Batch: 438/1186 loss: 0.9917129874229431\n",
      "Epoch: 0 Batch: 439/1186 loss: 1.051517367362976\n",
      "Epoch: 0 Batch: 440/1186 loss: 1.034443974494934\n",
      "Epoch: 0 Batch: 441/1186 loss: 0.9590209126472473\n",
      "Epoch: 0 Batch: 442/1186 loss: 0.993781566619873\n",
      "Epoch: 0 Batch: 443/1186 loss: 0.9311352968215942\n",
      "Epoch: 0 Batch: 444/1186 loss: 0.9790259003639221\n",
      "Epoch: 0 Batch: 445/1186 loss: 0.9723170399665833\n",
      "Epoch: 0 Batch: 446/1186 loss: 0.9858134984970093\n",
      "Epoch: 0 Batch: 447/1186 loss: 0.9071581363677979\n",
      "Epoch: 0 Batch: 448/1186 loss: 0.9675828814506531\n",
      "Epoch: 0 Batch: 449/1186 loss: 1.0538969039916992\n",
      "Epoch: 0 Batch: 450/1186 loss: 1.074645757675171\n",
      "Epoch: 0 Batch: 451/1186 loss: 1.003816843032837\n",
      "Epoch: 0 Batch: 452/1186 loss: 0.8631413578987122\n",
      "Epoch: 0 Batch: 453/1186 loss: 0.8583143353462219\n",
      "Epoch: 0 Batch: 454/1186 loss: 0.9083672761917114\n",
      "Epoch: 0 Batch: 455/1186 loss: 0.9070430397987366\n",
      "Epoch: 0 Batch: 456/1186 loss: 0.896084725856781\n",
      "Epoch: 0 Batch: 457/1186 loss: 0.9980972409248352\n",
      "Epoch: 0 Batch: 458/1186 loss: 0.9491644501686096\n",
      "Epoch: 0 Batch: 459/1186 loss: 0.9985610246658325\n",
      "Epoch: 0 Batch: 460/1186 loss: 0.9103825688362122\n",
      "Epoch: 0 Batch: 461/1186 loss: 0.8272218108177185\n",
      "Epoch: 0 Batch: 462/1186 loss: 0.8550207018852234\n",
      "Epoch: 0 Batch: 463/1186 loss: 0.9607274532318115\n",
      "Epoch: 0 Batch: 464/1186 loss: 1.0601022243499756\n",
      "Epoch: 0 Batch: 465/1186 loss: 0.9511851668357849\n",
      "Epoch: 0 Batch: 466/1186 loss: 0.9367926120758057\n",
      "Epoch: 0 Batch: 467/1186 loss: 0.8995433449745178\n",
      "Epoch: 0 Batch: 468/1186 loss: 0.898730993270874\n",
      "Epoch: 0 Batch: 469/1186 loss: 0.9374992847442627\n",
      "Epoch: 0 Batch: 470/1186 loss: 0.9451799988746643\n",
      "Epoch: 0 Batch: 471/1186 loss: 0.9064456224441528\n",
      "Epoch: 0 Batch: 472/1186 loss: 0.8739792704582214\n",
      "Epoch: 0 Batch: 473/1186 loss: 0.8399825096130371\n",
      "Epoch: 0 Batch: 474/1186 loss: 0.9362536668777466\n",
      "Epoch: 0 Batch: 475/1186 loss: 0.9372187256813049\n",
      "Epoch: 0 Batch: 476/1186 loss: 0.8795043230056763\n",
      "Epoch: 0 Batch: 477/1186 loss: 0.942308783531189\n",
      "Epoch: 0 Batch: 478/1186 loss: 0.8377121090888977\n",
      "Epoch: 0 Batch: 479/1186 loss: 0.9060726165771484\n",
      "Epoch: 0 Batch: 480/1186 loss: 1.0460002422332764\n",
      "Epoch: 0 Batch: 481/1186 loss: 0.9310685992240906\n",
      "Epoch: 0 Batch: 482/1186 loss: 1.0045385360717773\n",
      "Epoch: 0 Batch: 483/1186 loss: 0.8952142596244812\n",
      "Epoch: 0 Batch: 484/1186 loss: 0.8146430850028992\n",
      "Epoch: 0 Batch: 485/1186 loss: 0.8679943680763245\n",
      "Epoch: 0 Batch: 486/1186 loss: 0.9402150511741638\n",
      "Epoch: 0 Batch: 487/1186 loss: 1.1632616519927979\n",
      "Epoch: 0 Batch: 488/1186 loss: 0.7892488241195679\n",
      "Epoch: 0 Batch: 489/1186 loss: 0.9277239441871643\n",
      "Epoch: 0 Batch: 490/1186 loss: 0.9855469465255737\n",
      "Epoch: 0 Batch: 491/1186 loss: 0.9351884126663208\n",
      "Epoch: 0 Batch: 492/1186 loss: 0.9398287534713745\n",
      "Epoch: 0 Batch: 493/1186 loss: 0.9585651755332947\n",
      "Epoch: 0 Batch: 494/1186 loss: 0.9560744166374207\n",
      "Epoch: 0 Batch: 495/1186 loss: 0.9145686626434326\n",
      "Epoch: 0 Batch: 496/1186 loss: 0.9338124990463257\n",
      "Epoch: 0 Batch: 497/1186 loss: 0.9111482501029968\n",
      "Epoch: 0 Batch: 498/1186 loss: 0.8979824185371399\n",
      "Epoch: 0 Batch: 499/1186 loss: 0.970865786075592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 500/1186 loss: 0.7488059997558594\n",
      "Epoch: 0 Batch: 501/1186 loss: 0.7359766364097595\n",
      "Epoch: 0 Batch: 502/1186 loss: 0.786503791809082\n",
      "Epoch: 0 Batch: 503/1186 loss: 0.7827017307281494\n",
      "Epoch: 0 Batch: 504/1186 loss: 0.9035792946815491\n",
      "Epoch: 0 Batch: 505/1186 loss: 1.0364521741867065\n",
      "Epoch: 0 Batch: 506/1186 loss: 1.0465118885040283\n",
      "Epoch: 0 Batch: 507/1186 loss: 0.9953327178955078\n",
      "Epoch: 0 Batch: 508/1186 loss: 0.7948316335678101\n",
      "Epoch: 0 Batch: 509/1186 loss: 0.9006295204162598\n",
      "Epoch: 0 Batch: 510/1186 loss: 0.9541582465171814\n",
      "Epoch: 0 Batch: 511/1186 loss: 0.9283533096313477\n",
      "Epoch: 0 Batch: 512/1186 loss: 0.9761582612991333\n",
      "Epoch: 0 Batch: 513/1186 loss: 0.8816893696784973\n",
      "Epoch: 0 Batch: 514/1186 loss: 0.8196895718574524\n",
      "Epoch: 0 Batch: 515/1186 loss: 0.8546434044837952\n",
      "Epoch: 0 Batch: 516/1186 loss: 0.8404038548469543\n",
      "Epoch: 0 Batch: 517/1186 loss: 0.8452087044715881\n",
      "Epoch: 0 Batch: 518/1186 loss: 0.9295656085014343\n",
      "Epoch: 0 Batch: 519/1186 loss: 0.6544380784034729\n",
      "Epoch: 0 Batch: 520/1186 loss: 1.0136290788650513\n",
      "Epoch: 0 Batch: 521/1186 loss: 0.8807905316352844\n",
      "Epoch: 0 Batch: 522/1186 loss: 1.0289663076400757\n",
      "Epoch: 0 Batch: 523/1186 loss: 0.9537776112556458\n",
      "Epoch: 0 Batch: 524/1186 loss: 1.0015623569488525\n",
      "Epoch: 0 Batch: 525/1186 loss: 0.8565818667411804\n",
      "Epoch: 0 Batch: 526/1186 loss: 0.8336950540542603\n",
      "Epoch: 0 Batch: 527/1186 loss: 0.8373447060585022\n",
      "Epoch: 0 Batch: 528/1186 loss: 0.9269574880599976\n",
      "Epoch: 0 Batch: 529/1186 loss: 0.9140937328338623\n",
      "Epoch: 0 Batch: 530/1186 loss: 0.9381084442138672\n",
      "Epoch: 0 Batch: 531/1186 loss: 0.9130288362503052\n",
      "Epoch: 0 Batch: 532/1186 loss: 0.9338654279708862\n",
      "Epoch: 0 Batch: 533/1186 loss: 0.7765883207321167\n",
      "Epoch: 0 Batch: 534/1186 loss: 0.9765142202377319\n",
      "Epoch: 0 Batch: 535/1186 loss: 0.9256929159164429\n",
      "Epoch: 0 Batch: 536/1186 loss: 0.9211190938949585\n",
      "Epoch: 0 Batch: 537/1186 loss: 1.0624644756317139\n",
      "Epoch: 0 Batch: 538/1186 loss: 0.926681399345398\n",
      "Epoch: 0 Batch: 539/1186 loss: 0.8425455093383789\n",
      "Epoch: 0 Batch: 540/1186 loss: 0.84613436460495\n",
      "Epoch: 0 Batch: 541/1186 loss: 0.8914161324501038\n",
      "Epoch: 0 Batch: 542/1186 loss: 0.8764500617980957\n",
      "Epoch: 0 Batch: 543/1186 loss: 0.9386196732521057\n",
      "Epoch: 0 Batch: 544/1186 loss: 0.8073121309280396\n",
      "Epoch: 0 Batch: 545/1186 loss: 0.9507729411125183\n",
      "Epoch: 0 Batch: 546/1186 loss: 0.9710072875022888\n",
      "Epoch: 0 Batch: 547/1186 loss: 0.8448499441146851\n",
      "Epoch: 0 Batch: 548/1186 loss: 0.7884341478347778\n",
      "Epoch: 0 Batch: 549/1186 loss: 0.897955060005188\n",
      "Epoch: 0 Batch: 550/1186 loss: 0.870866060256958\n",
      "Epoch: 0 Batch: 551/1186 loss: 1.0747976303100586\n",
      "Epoch: 0 Batch: 552/1186 loss: 1.133005142211914\n",
      "Epoch: 0 Batch: 553/1186 loss: 0.8711830973625183\n",
      "Epoch: 0 Batch: 554/1186 loss: 0.7080074548721313\n",
      "Epoch: 0 Batch: 555/1186 loss: 0.7612519860267639\n",
      "Epoch: 0 Batch: 556/1186 loss: 0.8202264308929443\n",
      "Epoch: 0 Batch: 557/1186 loss: 0.9545990824699402\n",
      "Epoch: 0 Batch: 558/1186 loss: 0.7918866276741028\n",
      "Epoch: 0 Batch: 559/1186 loss: 0.8495352864265442\n",
      "Epoch: 0 Batch: 560/1186 loss: 0.7070966362953186\n",
      "Epoch: 0 Batch: 561/1186 loss: 0.7823774814605713\n",
      "Epoch: 0 Batch: 562/1186 loss: 0.9249228239059448\n",
      "Epoch: 0 Batch: 563/1186 loss: 0.985965371131897\n",
      "Epoch: 0 Batch: 564/1186 loss: 0.9618868827819824\n",
      "Epoch: 0 Batch: 565/1186 loss: 0.756833553314209\n",
      "Epoch: 0 Batch: 566/1186 loss: 0.7955257296562195\n",
      "Epoch: 0 Batch: 567/1186 loss: 0.8047453165054321\n",
      "Epoch: 0 Batch: 568/1186 loss: 0.901417076587677\n",
      "Epoch: 0 Batch: 569/1186 loss: 0.9183324575424194\n",
      "Epoch: 0 Batch: 570/1186 loss: 0.7681922316551208\n",
      "Epoch: 0 Batch: 571/1186 loss: 0.8125065565109253\n",
      "Epoch: 0 Batch: 572/1186 loss: 0.7806505560874939\n",
      "Epoch: 0 Batch: 573/1186 loss: 0.8381502628326416\n",
      "Epoch: 0 Batch: 574/1186 loss: 0.8094437718391418\n",
      "Epoch: 0 Batch: 575/1186 loss: 0.8315341472625732\n",
      "Epoch: 0 Batch: 576/1186 loss: 0.7784562706947327\n",
      "Epoch: 0 Batch: 577/1186 loss: 0.8947644233703613\n",
      "Epoch: 0 Batch: 578/1186 loss: 0.7707014679908752\n",
      "Epoch: 0 Batch: 579/1186 loss: 0.8783330917358398\n",
      "Epoch: 0 Batch: 580/1186 loss: 0.8698585033416748\n",
      "Epoch: 0 Batch: 581/1186 loss: 0.6796449422836304\n",
      "Epoch: 0 Batch: 582/1186 loss: 0.8138617873191833\n",
      "Epoch: 0 Batch: 583/1186 loss: 0.8047894239425659\n",
      "Epoch: 0 Batch: 584/1186 loss: 0.7241330742835999\n",
      "Epoch: 0 Batch: 585/1186 loss: 0.771772563457489\n",
      "Epoch: 0 Batch: 586/1186 loss: 0.7872100472450256\n",
      "Epoch: 0 Batch: 587/1186 loss: 0.7938594222068787\n",
      "Epoch: 0 Batch: 588/1186 loss: 0.796973705291748\n",
      "Epoch: 0 Batch: 589/1186 loss: 0.9098353981971741\n",
      "Epoch: 0 Batch: 590/1186 loss: 0.7413098812103271\n",
      "Epoch: 0 Batch: 591/1186 loss: 0.6241324543952942\n",
      "Epoch: 0 Batch: 592/1186 loss: 0.7847326993942261\n",
      "Epoch: 0 Batch: 593/1186 loss: 0.7969213724136353\n",
      "Epoch: 0 Batch: 594/1186 loss: 0.7337266206741333\n",
      "Epoch: 0 Batch: 595/1186 loss: 0.8931313157081604\n",
      "Epoch: 0 Batch: 596/1186 loss: 0.8750402331352234\n",
      "Epoch: 0 Batch: 597/1186 loss: 0.8815932869911194\n",
      "Epoch: 0 Batch: 598/1186 loss: 0.9888764023780823\n",
      "Epoch: 0 Batch: 599/1186 loss: 0.7825589179992676\n",
      "Epoch: 0 Batch: 600/1186 loss: 0.7943371534347534\n",
      "Epoch: 0 Batch: 601/1186 loss: 0.7931438088417053\n",
      "Epoch: 0 Batch: 602/1186 loss: 0.6002969145774841\n",
      "Epoch: 0 Batch: 603/1186 loss: 0.7247554659843445\n",
      "Epoch: 0 Batch: 604/1186 loss: 0.7105310559272766\n",
      "Epoch: 0 Batch: 605/1186 loss: 0.7475091814994812\n",
      "Epoch: 0 Batch: 606/1186 loss: 0.8624917268753052\n",
      "Epoch: 0 Batch: 607/1186 loss: 0.8113048672676086\n",
      "Epoch: 0 Batch: 608/1186 loss: 0.9149043560028076\n",
      "Epoch: 0 Batch: 609/1186 loss: 0.9702528119087219\n",
      "Epoch: 0 Batch: 610/1186 loss: 0.8140265941619873\n",
      "Epoch: 0 Batch: 611/1186 loss: 0.7906485795974731\n",
      "Epoch: 0 Batch: 612/1186 loss: 0.8773473501205444\n",
      "Epoch: 0 Batch: 613/1186 loss: 0.8033533096313477\n",
      "Epoch: 0 Batch: 614/1186 loss: 0.8179858922958374\n",
      "Epoch: 0 Batch: 615/1186 loss: 0.9303338527679443\n",
      "Epoch: 0 Batch: 616/1186 loss: 0.7627834677696228\n",
      "Epoch: 0 Batch: 617/1186 loss: 0.7741527557373047\n",
      "Epoch: 0 Batch: 618/1186 loss: 0.8696312308311462\n",
      "Epoch: 0 Batch: 619/1186 loss: 0.9316995739936829\n",
      "Epoch: 0 Batch: 620/1186 loss: 0.9380391836166382\n",
      "Epoch: 0 Batch: 621/1186 loss: 0.9781090617179871\n",
      "Epoch: 0 Batch: 622/1186 loss: 0.821486234664917\n",
      "Epoch: 0 Batch: 623/1186 loss: 0.7908638715744019\n",
      "Epoch: 0 Batch: 624/1186 loss: 0.8262335062026978\n",
      "Epoch: 0 Batch: 625/1186 loss: 0.8963841199874878\n",
      "Epoch: 0 Batch: 626/1186 loss: 0.8349632620811462\n",
      "Epoch: 0 Batch: 627/1186 loss: 0.8856035470962524\n",
      "Epoch: 0 Batch: 628/1186 loss: 0.890967845916748\n",
      "Epoch: 0 Batch: 629/1186 loss: 0.81239914894104\n",
      "Epoch: 0 Batch: 630/1186 loss: 0.8995280861854553\n",
      "Epoch: 0 Batch: 631/1186 loss: 0.8193755149841309\n",
      "Epoch: 0 Batch: 632/1186 loss: 0.6608773469924927\n",
      "Epoch: 0 Batch: 633/1186 loss: 0.7364110350608826\n",
      "Epoch: 0 Batch: 634/1186 loss: 0.7253517508506775\n",
      "Epoch: 0 Batch: 635/1186 loss: 0.8577885627746582\n",
      "Epoch: 0 Batch: 636/1186 loss: 0.7993239164352417\n",
      "Epoch: 0 Batch: 637/1186 loss: 0.6659351587295532\n",
      "Epoch: 0 Batch: 638/1186 loss: 0.8853938579559326\n",
      "Epoch: 0 Batch: 639/1186 loss: 0.7667518258094788\n",
      "Epoch: 0 Batch: 640/1186 loss: 0.8860359191894531\n",
      "Epoch: 0 Batch: 641/1186 loss: 0.8889024257659912\n",
      "Epoch: 0 Batch: 642/1186 loss: 0.7439555525779724\n",
      "Epoch: 0 Batch: 643/1186 loss: 0.753212034702301\n",
      "Epoch: 0 Batch: 644/1186 loss: 0.6498126983642578\n",
      "Epoch: 0 Batch: 645/1186 loss: 0.7036051750183105\n",
      "Epoch: 0 Batch: 646/1186 loss: 0.718288004398346\n",
      "Epoch: 0 Batch: 647/1186 loss: 0.7414321303367615\n",
      "Epoch: 0 Batch: 648/1186 loss: 0.7214318513870239\n",
      "Epoch: 0 Batch: 649/1186 loss: 0.785668134689331\n",
      "Epoch: 0 Batch: 650/1186 loss: 0.7905132174491882\n",
      "Epoch: 0 Batch: 651/1186 loss: 0.7539010047912598\n",
      "Epoch: 0 Batch: 652/1186 loss: 0.727087140083313\n",
      "Epoch: 0 Batch: 653/1186 loss: 0.7040700316429138\n",
      "Epoch: 0 Batch: 654/1186 loss: 0.7219600677490234\n",
      "Epoch: 0 Batch: 655/1186 loss: 0.8339452743530273\n",
      "Epoch: 0 Batch: 656/1186 loss: 0.7879669666290283\n",
      "Epoch: 0 Batch: 657/1186 loss: 0.6692147254943848\n",
      "Epoch: 0 Batch: 658/1186 loss: 0.558538556098938\n",
      "Epoch: 0 Batch: 659/1186 loss: 0.6557563543319702\n",
      "Epoch: 0 Batch: 660/1186 loss: 0.7370584607124329\n",
      "Epoch: 0 Batch: 661/1186 loss: 0.7284789085388184\n",
      "Epoch: 0 Batch: 662/1186 loss: 0.8193165063858032\n",
      "Epoch: 0 Batch: 663/1186 loss: 0.6863716840744019\n",
      "Epoch: 0 Batch: 664/1186 loss: 0.7696628570556641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 665/1186 loss: 0.8294754028320312\n",
      "Epoch: 0 Batch: 666/1186 loss: 0.8171490430831909\n",
      "Epoch: 0 Batch: 667/1186 loss: 0.8295988440513611\n",
      "Epoch: 0 Batch: 668/1186 loss: 0.7374513149261475\n",
      "Epoch: 0 Batch: 669/1186 loss: 0.7949957251548767\n",
      "Epoch: 0 Batch: 670/1186 loss: 0.7766125202178955\n",
      "Epoch: 0 Batch: 671/1186 loss: 0.7071159482002258\n",
      "Epoch: 0 Batch: 672/1186 loss: 0.8437638282775879\n",
      "Epoch: 0 Batch: 673/1186 loss: 0.9995682835578918\n",
      "Epoch: 0 Batch: 674/1186 loss: 0.7566387057304382\n",
      "Epoch: 0 Batch: 675/1186 loss: 0.92465740442276\n",
      "Epoch: 0 Batch: 676/1186 loss: 0.676298201084137\n",
      "Epoch: 0 Batch: 677/1186 loss: 0.7193630337715149\n",
      "Epoch: 0 Batch: 678/1186 loss: 0.6787469983100891\n",
      "Epoch: 0 Batch: 679/1186 loss: 0.8419257998466492\n",
      "Epoch: 0 Batch: 680/1186 loss: 0.8735013604164124\n",
      "Epoch: 0 Batch: 681/1186 loss: 0.7213037610054016\n",
      "Epoch: 0 Batch: 682/1186 loss: 0.768744945526123\n",
      "Epoch: 0 Batch: 683/1186 loss: 0.6649228930473328\n",
      "Epoch: 0 Batch: 684/1186 loss: 0.7435761094093323\n",
      "Epoch: 0 Batch: 685/1186 loss: 0.6324849724769592\n",
      "Epoch: 0 Batch: 686/1186 loss: 0.5859679579734802\n",
      "Epoch: 0 Batch: 687/1186 loss: 0.6433635354042053\n",
      "Epoch: 0 Batch: 688/1186 loss: 0.7547456622123718\n",
      "Epoch: 0 Batch: 689/1186 loss: 0.7687889337539673\n",
      "Epoch: 0 Batch: 690/1186 loss: 0.7924240827560425\n",
      "Epoch: 0 Batch: 691/1186 loss: 0.6126844882965088\n",
      "Epoch: 0 Batch: 692/1186 loss: 0.6879408359527588\n",
      "Epoch: 0 Batch: 693/1186 loss: 0.7190624475479126\n",
      "Epoch: 0 Batch: 694/1186 loss: 0.8348439931869507\n",
      "Epoch: 0 Batch: 695/1186 loss: 0.8152822256088257\n",
      "Epoch: 0 Batch: 696/1186 loss: 0.860873281955719\n",
      "Epoch: 0 Batch: 697/1186 loss: 0.9404699206352234\n",
      "Epoch: 0 Batch: 698/1186 loss: 0.8568788170814514\n",
      "Epoch: 0 Batch: 699/1186 loss: 0.7980741858482361\n",
      "Epoch: 0 Batch: 700/1186 loss: 0.7986539006233215\n",
      "Epoch: 0 Batch: 701/1186 loss: 0.7990159392356873\n",
      "Epoch: 0 Batch: 702/1186 loss: 0.8673719167709351\n",
      "Epoch: 0 Batch: 703/1186 loss: 0.65610271692276\n",
      "Epoch: 0 Batch: 704/1186 loss: 0.6997489929199219\n",
      "Epoch: 0 Batch: 705/1186 loss: 0.793512225151062\n",
      "Epoch: 0 Batch: 706/1186 loss: 0.8590080738067627\n",
      "Epoch: 0 Batch: 707/1186 loss: 0.875198483467102\n",
      "Epoch: 0 Batch: 708/1186 loss: 0.8160657286643982\n",
      "Epoch: 0 Batch: 709/1186 loss: 0.8663034439086914\n",
      "Epoch: 0 Batch: 710/1186 loss: 0.706421434879303\n",
      "Epoch: 0 Batch: 711/1186 loss: 0.7726955413818359\n",
      "Epoch: 0 Batch: 712/1186 loss: 0.7464709281921387\n",
      "Epoch: 0 Batch: 713/1186 loss: 0.8682574033737183\n",
      "Epoch: 0 Batch: 714/1186 loss: 0.8935259580612183\n",
      "Epoch: 0 Batch: 715/1186 loss: 0.830250084400177\n",
      "Epoch: 0 Batch: 716/1186 loss: 0.6761836409568787\n",
      "Epoch: 0 Batch: 717/1186 loss: 0.8044825196266174\n",
      "Epoch: 0 Batch: 718/1186 loss: 0.6844401955604553\n",
      "Epoch: 0 Batch: 719/1186 loss: 0.7739189267158508\n",
      "Epoch: 0 Batch: 720/1186 loss: 0.7819667458534241\n",
      "Epoch: 0 Batch: 721/1186 loss: 0.7702983021736145\n",
      "Epoch: 0 Batch: 722/1186 loss: 0.7880527973175049\n",
      "Epoch: 0 Batch: 723/1186 loss: 0.8876067996025085\n",
      "Epoch: 0 Batch: 724/1186 loss: 0.8576191067695618\n",
      "Epoch: 0 Batch: 725/1186 loss: 0.7014995813369751\n",
      "Epoch: 0 Batch: 726/1186 loss: 0.8038228750228882\n",
      "Epoch: 0 Batch: 727/1186 loss: 0.7596505880355835\n",
      "Epoch: 0 Batch: 728/1186 loss: 0.7582532167434692\n",
      "Epoch: 0 Batch: 729/1186 loss: 0.758223831653595\n",
      "Epoch: 0 Batch: 730/1186 loss: 0.7803484797477722\n",
      "Epoch: 0 Batch: 731/1186 loss: 0.700830340385437\n",
      "Epoch: 0 Batch: 732/1186 loss: 0.7517428994178772\n",
      "Epoch: 0 Batch: 733/1186 loss: 0.7858706116676331\n",
      "Epoch: 0 Batch: 734/1186 loss: 0.7834484577178955\n",
      "Epoch: 0 Batch: 735/1186 loss: 0.7585805654525757\n",
      "Epoch: 0 Batch: 736/1186 loss: 0.6653294563293457\n",
      "Epoch: 0 Batch: 737/1186 loss: 0.6836767196655273\n",
      "Epoch: 0 Batch: 738/1186 loss: 0.7188820242881775\n",
      "Epoch: 0 Batch: 739/1186 loss: 0.8129301071166992\n",
      "Epoch: 0 Batch: 740/1186 loss: 0.8057833313941956\n",
      "Epoch: 0 Batch: 741/1186 loss: 0.7117975354194641\n",
      "Epoch: 0 Batch: 742/1186 loss: 0.7944311499595642\n",
      "Epoch: 0 Batch: 743/1186 loss: 0.8441069722175598\n",
      "Epoch: 0 Batch: 744/1186 loss: 0.7685844302177429\n",
      "Epoch: 0 Batch: 745/1186 loss: 0.610380232334137\n",
      "Epoch: 0 Batch: 746/1186 loss: 0.6938810348510742\n",
      "Epoch: 0 Batch: 747/1186 loss: 0.8437817692756653\n",
      "Epoch: 0 Batch: 748/1186 loss: 0.7268092632293701\n",
      "Epoch: 0 Batch: 749/1186 loss: 0.742603063583374\n",
      "Epoch: 0 Batch: 750/1186 loss: 0.6288477778434753\n",
      "Epoch: 0 Batch: 751/1186 loss: 0.6509120464324951\n",
      "Epoch: 0 Batch: 752/1186 loss: 0.6904791593551636\n",
      "Epoch: 0 Batch: 753/1186 loss: 0.6393654346466064\n",
      "Epoch: 0 Batch: 754/1186 loss: 0.678349494934082\n",
      "Epoch: 0 Batch: 755/1186 loss: 0.7038639783859253\n",
      "Epoch: 0 Batch: 756/1186 loss: 0.6927211880683899\n",
      "Epoch: 0 Batch: 757/1186 loss: 0.7482845187187195\n",
      "Epoch: 0 Batch: 758/1186 loss: 0.7344386577606201\n",
      "Epoch: 0 Batch: 759/1186 loss: 0.7123401761054993\n",
      "Epoch: 0 Batch: 760/1186 loss: 0.7371306419372559\n",
      "Epoch: 0 Batch: 761/1186 loss: 0.7869169116020203\n",
      "Epoch: 0 Batch: 762/1186 loss: 0.7424091100692749\n",
      "Epoch: 0 Batch: 763/1186 loss: 0.6624869704246521\n",
      "Epoch: 0 Batch: 764/1186 loss: 0.8093441128730774\n",
      "Epoch: 0 Batch: 765/1186 loss: 0.7889710068702698\n",
      "Epoch: 0 Batch: 766/1186 loss: 0.7435463070869446\n",
      "Epoch: 0 Batch: 767/1186 loss: 0.8416426777839661\n",
      "Epoch: 0 Batch: 768/1186 loss: 0.7270527482032776\n",
      "Epoch: 0 Batch: 769/1186 loss: 0.8413850665092468\n",
      "Epoch: 0 Batch: 770/1186 loss: 0.6193669438362122\n",
      "Epoch: 0 Batch: 771/1186 loss: 0.6964584589004517\n",
      "Epoch: 0 Batch: 772/1186 loss: 0.6696060299873352\n",
      "Epoch: 0 Batch: 773/1186 loss: 0.7477412819862366\n",
      "Epoch: 0 Batch: 774/1186 loss: 0.6935839653015137\n",
      "Epoch: 0 Batch: 775/1186 loss: 0.6706392765045166\n",
      "Epoch: 0 Batch: 776/1186 loss: 0.6913625001907349\n",
      "Epoch: 0 Batch: 777/1186 loss: 0.74018794298172\n",
      "Epoch: 0 Batch: 778/1186 loss: 0.7805619239807129\n",
      "Epoch: 0 Batch: 779/1186 loss: 0.7734913229942322\n",
      "Epoch: 0 Batch: 780/1186 loss: 0.7450565695762634\n",
      "Epoch: 0 Batch: 781/1186 loss: 0.6592766046524048\n",
      "Epoch: 0 Batch: 782/1186 loss: 0.6437957882881165\n",
      "Epoch: 0 Batch: 783/1186 loss: 0.6580511331558228\n",
      "Epoch: 0 Batch: 784/1186 loss: 0.6892184615135193\n",
      "Epoch: 0 Batch: 785/1186 loss: 0.749168336391449\n",
      "Epoch: 0 Batch: 786/1186 loss: 0.6961724758148193\n",
      "Epoch: 0 Batch: 787/1186 loss: 0.678561270236969\n",
      "Epoch: 0 Batch: 788/1186 loss: 0.5989120006561279\n",
      "Epoch: 0 Batch: 789/1186 loss: 0.6456558704376221\n",
      "Epoch: 0 Batch: 790/1186 loss: 0.7311148047447205\n",
      "Epoch: 0 Batch: 791/1186 loss: 0.8404932618141174\n",
      "Epoch: 0 Batch: 792/1186 loss: 0.853357195854187\n",
      "Epoch: 0 Batch: 793/1186 loss: 0.7553752660751343\n",
      "Epoch: 0 Batch: 794/1186 loss: 0.7310671210289001\n",
      "Epoch: 0 Batch: 795/1186 loss: 0.7387051582336426\n",
      "Epoch: 0 Batch: 796/1186 loss: 0.6748006343841553\n",
      "Epoch: 0 Batch: 797/1186 loss: 0.7758507132530212\n",
      "Epoch: 0 Batch: 798/1186 loss: 0.7036346793174744\n",
      "Epoch: 0 Batch: 799/1186 loss: 0.8062028288841248\n",
      "Epoch: 0 Batch: 800/1186 loss: 0.6832113862037659\n",
      "Epoch: 0 Batch: 801/1186 loss: 0.6724269390106201\n",
      "Epoch: 0 Batch: 802/1186 loss: 0.6872355341911316\n",
      "Epoch: 0 Batch: 803/1186 loss: 0.8167301416397095\n",
      "Epoch: 0 Batch: 804/1186 loss: 0.6843308806419373\n",
      "Epoch: 0 Batch: 805/1186 loss: 0.6070922017097473\n",
      "Epoch: 0 Batch: 806/1186 loss: 0.7324480414390564\n",
      "Epoch: 0 Batch: 807/1186 loss: 0.5983677506446838\n",
      "Epoch: 0 Batch: 808/1186 loss: 0.6838304400444031\n",
      "Epoch: 0 Batch: 809/1186 loss: 0.7004602551460266\n",
      "Epoch: 0 Batch: 810/1186 loss: 0.6731708645820618\n",
      "Epoch: 0 Batch: 811/1186 loss: 0.606164813041687\n",
      "Epoch: 0 Batch: 812/1186 loss: 0.6444380879402161\n",
      "Epoch: 0 Batch: 813/1186 loss: 0.5962935090065002\n",
      "Epoch: 0 Batch: 814/1186 loss: 0.8255327939987183\n",
      "Epoch: 0 Batch: 815/1186 loss: 0.7122802138328552\n",
      "Epoch: 0 Batch: 816/1186 loss: 0.7194967865943909\n",
      "Epoch: 0 Batch: 817/1186 loss: 0.7378109693527222\n",
      "Epoch: 0 Batch: 818/1186 loss: 0.7858418226242065\n",
      "Epoch: 0 Batch: 819/1186 loss: 0.5789755582809448\n",
      "Epoch: 0 Batch: 820/1186 loss: 0.6642428636550903\n",
      "Epoch: 0 Batch: 821/1186 loss: 0.7291114330291748\n",
      "Epoch: 0 Batch: 822/1186 loss: 0.6852608919143677\n",
      "Epoch: 0 Batch: 823/1186 loss: 0.723406195640564\n",
      "Epoch: 0 Batch: 824/1186 loss: 0.6615287661552429\n",
      "Epoch: 0 Batch: 825/1186 loss: 0.6627576947212219\n",
      "Epoch: 0 Batch: 826/1186 loss: 0.6795786619186401\n",
      "Epoch: 0 Batch: 827/1186 loss: 0.7273146510124207\n",
      "Epoch: 0 Batch: 828/1186 loss: 0.6804417371749878\n",
      "Epoch: 0 Batch: 829/1186 loss: 0.7735685110092163\n",
      "Epoch: 0 Batch: 830/1186 loss: 0.8053358793258667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 831/1186 loss: 0.6773982644081116\n",
      "Epoch: 0 Batch: 832/1186 loss: 0.6651740074157715\n",
      "Epoch: 0 Batch: 833/1186 loss: 0.8221526741981506\n",
      "Epoch: 0 Batch: 834/1186 loss: 0.8634192943572998\n",
      "Epoch: 0 Batch: 835/1186 loss: 0.8047369122505188\n",
      "Epoch: 0 Batch: 836/1186 loss: 0.7104342579841614\n",
      "Epoch: 0 Batch: 837/1186 loss: 0.6944653987884521\n",
      "Epoch: 0 Batch: 838/1186 loss: 0.7744914889335632\n",
      "Epoch: 0 Batch: 839/1186 loss: 0.7004911303520203\n",
      "Epoch: 0 Batch: 840/1186 loss: 0.7314794063568115\n",
      "Epoch: 0 Batch: 841/1186 loss: 0.660953164100647\n",
      "Epoch: 0 Batch: 842/1186 loss: 0.7046672701835632\n",
      "Epoch: 0 Batch: 843/1186 loss: 0.7220805287361145\n",
      "Epoch: 0 Batch: 844/1186 loss: 0.6429375410079956\n",
      "Epoch: 0 Batch: 845/1186 loss: 0.6914911270141602\n",
      "Epoch: 0 Batch: 846/1186 loss: 0.6488958597183228\n",
      "Epoch: 0 Batch: 847/1186 loss: 0.6733807325363159\n",
      "Epoch: 0 Batch: 848/1186 loss: 0.6356037259101868\n",
      "Epoch: 0 Batch: 849/1186 loss: 0.6716345548629761\n",
      "Epoch: 0 Batch: 850/1186 loss: 0.7138712406158447\n",
      "Epoch: 0 Batch: 851/1186 loss: 0.7534497380256653\n",
      "Epoch: 0 Batch: 852/1186 loss: 0.7435395121574402\n",
      "Epoch: 0 Batch: 853/1186 loss: 0.7556225657463074\n",
      "Epoch: 0 Batch: 854/1186 loss: 0.743466854095459\n",
      "Epoch: 0 Batch: 855/1186 loss: 0.6404780149459839\n",
      "Epoch: 0 Batch: 856/1186 loss: 0.5048244595527649\n",
      "Epoch: 0 Batch: 857/1186 loss: 0.5586278438568115\n",
      "Epoch: 0 Batch: 858/1186 loss: 0.5991384387016296\n",
      "Epoch: 0 Batch: 859/1186 loss: 0.5914174914360046\n",
      "Epoch: 0 Batch: 860/1186 loss: 0.6601744294166565\n",
      "Epoch: 0 Batch: 861/1186 loss: 0.5945305824279785\n",
      "Epoch: 0 Batch: 862/1186 loss: 0.6548824906349182\n",
      "Epoch: 0 Batch: 863/1186 loss: 0.6739755868911743\n",
      "Epoch: 0 Batch: 864/1186 loss: 0.8105676174163818\n",
      "Epoch: 0 Batch: 865/1186 loss: 0.7988153696060181\n",
      "Epoch: 0 Batch: 866/1186 loss: 0.6144769191741943\n",
      "Epoch: 0 Batch: 867/1186 loss: 0.6228187680244446\n",
      "Epoch: 0 Batch: 868/1186 loss: 0.447220116853714\n",
      "Epoch: 0 Batch: 869/1186 loss: 0.660499632358551\n",
      "Epoch: 0 Batch: 870/1186 loss: 0.7634806632995605\n",
      "Epoch: 0 Batch: 871/1186 loss: 0.7049594521522522\n",
      "Epoch: 0 Batch: 872/1186 loss: 0.726103663444519\n",
      "Epoch: 0 Batch: 873/1186 loss: 0.6244146823883057\n",
      "Epoch: 0 Batch: 874/1186 loss: 0.6316743493080139\n",
      "Epoch: 0 Batch: 875/1186 loss: 0.7323639988899231\n",
      "Epoch: 0 Batch: 876/1186 loss: 0.862592339515686\n",
      "Epoch: 0 Batch: 877/1186 loss: 0.6268852949142456\n",
      "Epoch: 0 Batch: 878/1186 loss: 0.5877407789230347\n",
      "Epoch: 0 Batch: 879/1186 loss: 0.6136730313301086\n",
      "Epoch: 0 Batch: 880/1186 loss: 0.765366792678833\n",
      "Epoch: 0 Batch: 881/1186 loss: 0.6317508220672607\n",
      "Epoch: 0 Batch: 882/1186 loss: 0.7255790829658508\n",
      "Epoch: 0 Batch: 883/1186 loss: 0.6563200354576111\n",
      "Epoch: 0 Batch: 884/1186 loss: 0.582399845123291\n",
      "Epoch: 0 Batch: 885/1186 loss: 0.6537351608276367\n",
      "Epoch: 0 Batch: 886/1186 loss: 0.702555239200592\n",
      "Epoch: 0 Batch: 887/1186 loss: 0.7206821441650391\n",
      "Epoch: 0 Batch: 888/1186 loss: 0.680431604385376\n",
      "Epoch: 0 Batch: 889/1186 loss: 0.6077820062637329\n",
      "Epoch: 0 Batch: 890/1186 loss: 0.5387859344482422\n",
      "Epoch: 0 Batch: 891/1186 loss: 0.6298525333404541\n",
      "Epoch: 0 Batch: 892/1186 loss: 0.6878953576087952\n",
      "Epoch: 0 Batch: 893/1186 loss: 0.5432792901992798\n",
      "Epoch: 0 Batch: 894/1186 loss: 0.6567039489746094\n",
      "Epoch: 0 Batch: 895/1186 loss: 0.7605324983596802\n",
      "Epoch: 0 Batch: 896/1186 loss: 0.714904248714447\n",
      "Epoch: 0 Batch: 897/1186 loss: 0.6282575130462646\n",
      "Epoch: 0 Batch: 898/1186 loss: 0.6291801333427429\n",
      "Epoch: 0 Batch: 899/1186 loss: 0.6306906342506409\n",
      "Epoch: 0 Batch: 900/1186 loss: 0.7718419432640076\n",
      "Epoch: 0 Batch: 901/1186 loss: 0.4907001256942749\n",
      "Epoch: 0 Batch: 902/1186 loss: 0.6449927091598511\n",
      "Epoch: 0 Batch: 903/1186 loss: 0.5869354009628296\n",
      "Epoch: 0 Batch: 904/1186 loss: 0.5057381391525269\n",
      "Epoch: 0 Batch: 905/1186 loss: 0.9215751886367798\n",
      "Epoch: 0 Batch: 906/1186 loss: 0.6883181929588318\n",
      "Epoch: 0 Batch: 907/1186 loss: 0.6386823654174805\n",
      "Epoch: 0 Batch: 908/1186 loss: 0.6259446144104004\n",
      "Epoch: 0 Batch: 909/1186 loss: 0.6631342172622681\n",
      "Epoch: 0 Batch: 910/1186 loss: 0.7142416834831238\n",
      "Epoch: 0 Batch: 911/1186 loss: 0.6381503939628601\n",
      "Epoch: 0 Batch: 912/1186 loss: 0.4871160686016083\n",
      "Epoch: 0 Batch: 913/1186 loss: 0.569725513458252\n",
      "Epoch: 0 Batch: 914/1186 loss: 0.6155598163604736\n",
      "Epoch: 0 Batch: 915/1186 loss: 0.8660796284675598\n",
      "Epoch: 0 Batch: 916/1186 loss: 0.7437070608139038\n",
      "Epoch: 0 Batch: 917/1186 loss: 0.6675341129302979\n",
      "Epoch: 0 Batch: 918/1186 loss: 0.596276044845581\n",
      "Epoch: 0 Batch: 919/1186 loss: 0.5311346650123596\n",
      "Epoch: 0 Batch: 920/1186 loss: 0.5383501052856445\n",
      "Epoch: 0 Batch: 921/1186 loss: 0.6472028493881226\n",
      "Epoch: 0 Batch: 922/1186 loss: 0.6141136288642883\n",
      "Epoch: 0 Batch: 923/1186 loss: 0.5735657215118408\n",
      "Epoch: 0 Batch: 924/1186 loss: 0.7144381999969482\n",
      "Epoch: 0 Batch: 925/1186 loss: 0.5229886770248413\n",
      "Epoch: 0 Batch: 926/1186 loss: 0.6306803822517395\n",
      "Epoch: 0 Batch: 927/1186 loss: 0.5205132961273193\n",
      "Epoch: 0 Batch: 928/1186 loss: 0.7088967561721802\n",
      "Epoch: 0 Batch: 929/1186 loss: 0.7142829895019531\n",
      "Epoch: 0 Batch: 930/1186 loss: 0.5673062801361084\n",
      "Epoch: 0 Batch: 931/1186 loss: 0.7116957306861877\n",
      "Epoch: 0 Batch: 932/1186 loss: 0.7104381918907166\n",
      "Epoch: 0 Batch: 933/1186 loss: 0.6410894393920898\n",
      "Epoch: 0 Batch: 934/1186 loss: 0.6221616268157959\n",
      "Epoch: 0 Batch: 935/1186 loss: 0.6597402691841125\n",
      "Epoch: 0 Batch: 936/1186 loss: 0.6794607639312744\n",
      "Epoch: 0 Batch: 937/1186 loss: 0.6650980114936829\n",
      "Epoch: 0 Batch: 938/1186 loss: 0.7270686626434326\n",
      "Epoch: 0 Batch: 939/1186 loss: 0.7092756628990173\n",
      "Epoch: 0 Batch: 940/1186 loss: 0.5486665368080139\n",
      "Epoch: 0 Batch: 941/1186 loss: 0.69948810338974\n",
      "Epoch: 0 Batch: 942/1186 loss: 0.7195241451263428\n",
      "Epoch: 0 Batch: 943/1186 loss: 0.6267279386520386\n",
      "Epoch: 0 Batch: 944/1186 loss: 0.7581839561462402\n",
      "Epoch: 0 Batch: 945/1186 loss: 0.6252163648605347\n",
      "Epoch: 0 Batch: 946/1186 loss: 0.5762049555778503\n",
      "Epoch: 0 Batch: 947/1186 loss: 0.8072721362113953\n",
      "Epoch: 0 Batch: 948/1186 loss: 0.5880491733551025\n",
      "Epoch: 0 Batch: 949/1186 loss: 0.8110411763191223\n",
      "Epoch: 0 Batch: 950/1186 loss: 0.6320116519927979\n",
      "Epoch: 0 Batch: 951/1186 loss: 0.5838269591331482\n",
      "Epoch: 0 Batch: 952/1186 loss: 0.6449211239814758\n",
      "Epoch: 0 Batch: 953/1186 loss: 0.5780266523361206\n",
      "Epoch: 0 Batch: 954/1186 loss: 0.5762571096420288\n",
      "Epoch: 0 Batch: 955/1186 loss: 0.5929139256477356\n",
      "Epoch: 0 Batch: 956/1186 loss: 0.6605340838432312\n",
      "Epoch: 0 Batch: 957/1186 loss: 0.6336389183998108\n",
      "Epoch: 0 Batch: 958/1186 loss: 0.627286970615387\n",
      "Epoch: 0 Batch: 959/1186 loss: 0.7607747912406921\n",
      "Epoch: 0 Batch: 960/1186 loss: 0.6098668575286865\n",
      "Epoch: 0 Batch: 961/1186 loss: 0.5137993693351746\n",
      "Epoch: 0 Batch: 962/1186 loss: 0.5667328238487244\n",
      "Epoch: 0 Batch: 963/1186 loss: 0.5630795955657959\n",
      "Epoch: 0 Batch: 964/1186 loss: 0.47361454367637634\n",
      "Epoch: 0 Batch: 965/1186 loss: 0.4291914105415344\n",
      "Epoch: 0 Batch: 966/1186 loss: 0.647922694683075\n",
      "Epoch: 0 Batch: 967/1186 loss: 0.6770200729370117\n",
      "Epoch: 0 Batch: 968/1186 loss: 0.5910817980766296\n",
      "Epoch: 0 Batch: 969/1186 loss: 0.4622611403465271\n",
      "Epoch: 0 Batch: 970/1186 loss: 0.6715194582939148\n",
      "Epoch: 0 Batch: 971/1186 loss: 0.6055530309677124\n",
      "Epoch: 0 Batch: 972/1186 loss: 0.706142783164978\n",
      "Epoch: 0 Batch: 973/1186 loss: 0.8139271140098572\n",
      "Epoch: 0 Batch: 974/1186 loss: 0.7923430800437927\n",
      "Epoch: 0 Batch: 975/1186 loss: 0.6510939002037048\n",
      "Epoch: 0 Batch: 976/1186 loss: 0.6272814273834229\n",
      "Epoch: 0 Batch: 977/1186 loss: 0.4796124994754791\n",
      "Epoch: 0 Batch: 978/1186 loss: 0.5880897641181946\n",
      "Epoch: 0 Batch: 979/1186 loss: 0.5544531345367432\n",
      "Epoch: 0 Batch: 980/1186 loss: 0.5643303990364075\n",
      "Epoch: 0 Batch: 981/1186 loss: 0.6654698848724365\n",
      "Epoch: 0 Batch: 982/1186 loss: 0.5201630592346191\n",
      "Epoch: 0 Batch: 983/1186 loss: 0.8056510090827942\n",
      "Epoch: 0 Batch: 984/1186 loss: 0.8697572350502014\n",
      "Epoch: 0 Batch: 985/1186 loss: 0.8132718801498413\n",
      "Epoch: 0 Batch: 986/1186 loss: 0.7399072647094727\n",
      "Epoch: 0 Batch: 987/1186 loss: 0.5066331028938293\n",
      "Epoch: 0 Batch: 988/1186 loss: 0.5792096853256226\n",
      "Epoch: 0 Batch: 989/1186 loss: 0.5471813678741455\n",
      "Epoch: 0 Batch: 990/1186 loss: 0.5997083783149719\n",
      "Epoch: 0 Batch: 991/1186 loss: 0.40775221586227417\n",
      "Epoch: 0 Batch: 992/1186 loss: 0.6467424035072327\n",
      "Epoch: 0 Batch: 993/1186 loss: 0.6994620561599731\n",
      "Epoch: 0 Batch: 994/1186 loss: 0.622031569480896\n",
      "Epoch: 0 Batch: 995/1186 loss: 0.8071927428245544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 996/1186 loss: 0.5158782005310059\n",
      "Epoch: 0 Batch: 997/1186 loss: 0.5501611828804016\n",
      "Epoch: 0 Batch: 998/1186 loss: 0.6439496278762817\n",
      "Epoch: 0 Batch: 999/1186 loss: 0.5919916033744812\n",
      "Epoch: 0 Batch: 1000/1186 loss: 0.5637508630752563\n",
      "Epoch: 0 Batch: 1001/1186 loss: 0.6497008800506592\n",
      "Epoch: 0 Batch: 1002/1186 loss: 0.5812126994132996\n",
      "Epoch: 0 Batch: 1003/1186 loss: 0.6350045204162598\n",
      "Epoch: 0 Batch: 1004/1186 loss: 0.5963842868804932\n",
      "Epoch: 0 Batch: 1005/1186 loss: 0.6273561716079712\n",
      "Epoch: 0 Batch: 1006/1186 loss: 0.6541322469711304\n",
      "Epoch: 0 Batch: 1007/1186 loss: 0.532993495464325\n",
      "Epoch: 0 Batch: 1008/1186 loss: 0.5734787583351135\n",
      "Epoch: 0 Batch: 1009/1186 loss: 0.7045409083366394\n",
      "Epoch: 0 Batch: 1010/1186 loss: 0.5256373286247253\n",
      "Epoch: 0 Batch: 1011/1186 loss: 0.5662253499031067\n",
      "Epoch: 0 Batch: 1012/1186 loss: 0.6619151830673218\n",
      "Epoch: 0 Batch: 1013/1186 loss: 0.7213365435600281\n",
      "Epoch: 0 Batch: 1014/1186 loss: 0.5940865278244019\n",
      "Epoch: 0 Batch: 1015/1186 loss: 0.615371823310852\n",
      "Epoch: 0 Batch: 1016/1186 loss: 0.5117077827453613\n",
      "Epoch: 0 Batch: 1017/1186 loss: 0.5635126233100891\n",
      "Epoch: 0 Batch: 1018/1186 loss: 0.6521744728088379\n",
      "Epoch: 0 Batch: 1019/1186 loss: 0.6389065384864807\n",
      "Epoch: 0 Batch: 1020/1186 loss: 0.6865223050117493\n",
      "Epoch: 0 Batch: 1021/1186 loss: 0.6822036504745483\n",
      "Epoch: 0 Batch: 1022/1186 loss: 0.5548820495605469\n",
      "Epoch: 0 Batch: 1023/1186 loss: 0.6151182651519775\n",
      "Epoch: 0 Batch: 1024/1186 loss: 0.7677407264709473\n",
      "Epoch: 0 Batch: 1025/1186 loss: 0.661329448223114\n",
      "Epoch: 0 Batch: 1026/1186 loss: 0.8001368641853333\n",
      "Epoch: 0 Batch: 1027/1186 loss: 0.579155445098877\n",
      "Epoch: 0 Batch: 1028/1186 loss: 0.6155763864517212\n",
      "Epoch: 0 Batch: 1029/1186 loss: 0.6822559833526611\n",
      "Epoch: 0 Batch: 1030/1186 loss: 0.6320760846138\n",
      "Epoch: 0 Batch: 1031/1186 loss: 0.6428005695343018\n",
      "Epoch: 0 Batch: 1032/1186 loss: 0.7544385194778442\n",
      "Epoch: 0 Batch: 1033/1186 loss: 0.7249700427055359\n",
      "Epoch: 0 Batch: 1034/1186 loss: 0.5648930668830872\n",
      "Epoch: 0 Batch: 1035/1186 loss: 0.6454545259475708\n",
      "Epoch: 0 Batch: 1036/1186 loss: 0.5937710404396057\n",
      "Epoch: 0 Batch: 1037/1186 loss: 0.5882698893547058\n",
      "Epoch: 0 Batch: 1038/1186 loss: 0.5330489873886108\n",
      "Epoch: 0 Batch: 1039/1186 loss: 0.541991114616394\n",
      "Epoch: 0 Batch: 1040/1186 loss: 0.646440863609314\n",
      "Epoch: 0 Batch: 1041/1186 loss: 0.7382850646972656\n",
      "Epoch: 0 Batch: 1042/1186 loss: 0.6727855801582336\n",
      "Epoch: 0 Batch: 1043/1186 loss: 0.5452509522438049\n",
      "Epoch: 0 Batch: 1044/1186 loss: 0.5403821468353271\n",
      "Epoch: 0 Batch: 1045/1186 loss: 0.5808284878730774\n",
      "Epoch: 0 Batch: 1046/1186 loss: 0.606055498123169\n",
      "Epoch: 0 Batch: 1047/1186 loss: 0.5031518340110779\n",
      "Epoch: 0 Batch: 1048/1186 loss: 0.5129129886627197\n",
      "Epoch: 0 Batch: 1049/1186 loss: 0.647323489189148\n",
      "Epoch: 0 Batch: 1050/1186 loss: 0.5110560655593872\n",
      "Epoch: 0 Batch: 1051/1186 loss: 0.5124178528785706\n",
      "Epoch: 0 Batch: 1052/1186 loss: 0.6435558199882507\n",
      "Epoch: 0 Batch: 1053/1186 loss: 0.6166885495185852\n",
      "Epoch: 0 Batch: 1054/1186 loss: 0.5503310561180115\n",
      "Epoch: 0 Batch: 1055/1186 loss: 0.7538754940032959\n",
      "Epoch: 0 Batch: 1056/1186 loss: 0.5632381439208984\n",
      "Epoch: 0 Batch: 1057/1186 loss: 0.5018151998519897\n",
      "Epoch: 0 Batch: 1058/1186 loss: 0.5865103006362915\n",
      "Epoch: 0 Batch: 1059/1186 loss: 0.6920493245124817\n",
      "Epoch: 0 Batch: 1060/1186 loss: 0.6894983649253845\n",
      "Epoch: 0 Batch: 1061/1186 loss: 0.7079837918281555\n",
      "Epoch: 0 Batch: 1062/1186 loss: 0.6755561232566833\n",
      "Epoch: 0 Batch: 1063/1186 loss: 0.34385421872138977\n",
      "Epoch: 0 Batch: 1064/1186 loss: 0.566836416721344\n",
      "Epoch: 0 Batch: 1065/1186 loss: 0.5431078672409058\n",
      "Epoch: 0 Batch: 1066/1186 loss: 0.6338547468185425\n",
      "Epoch: 0 Batch: 1067/1186 loss: 0.5508334636688232\n",
      "Epoch: 0 Batch: 1068/1186 loss: 0.49859222769737244\n",
      "Epoch: 0 Batch: 1069/1186 loss: 0.5120867490768433\n",
      "Epoch: 0 Batch: 1070/1186 loss: 0.5686734914779663\n",
      "Epoch: 0 Batch: 1071/1186 loss: 0.5289433002471924\n",
      "Epoch: 0 Batch: 1072/1186 loss: 0.5566891431808472\n",
      "Epoch: 0 Batch: 1073/1186 loss: 0.6402574181556702\n",
      "Epoch: 0 Batch: 1074/1186 loss: 0.6955617666244507\n",
      "Epoch: 0 Batch: 1075/1186 loss: 0.5537412762641907\n",
      "Epoch: 0 Batch: 1076/1186 loss: 0.5170965790748596\n",
      "Epoch: 0 Batch: 1077/1186 loss: 0.524591326713562\n",
      "Epoch: 0 Batch: 1078/1186 loss: 0.5064444541931152\n",
      "Epoch: 0 Batch: 1079/1186 loss: 0.5169844627380371\n",
      "Epoch: 0 Batch: 1080/1186 loss: 0.4921357333660126\n",
      "Epoch: 0 Batch: 1081/1186 loss: 0.6737273931503296\n",
      "Epoch: 0 Batch: 1082/1186 loss: 0.5783668160438538\n",
      "Epoch: 0 Batch: 1083/1186 loss: 0.5853526592254639\n",
      "Epoch: 0 Batch: 1084/1186 loss: 0.6581505537033081\n",
      "Epoch: 0 Batch: 1085/1186 loss: 0.5659799575805664\n",
      "Epoch: 0 Batch: 1086/1186 loss: 0.6370545029640198\n",
      "Epoch: 0 Batch: 1087/1186 loss: 0.6703923940658569\n",
      "Epoch: 0 Batch: 1088/1186 loss: 0.561870813369751\n",
      "Epoch: 0 Batch: 1089/1186 loss: 0.6859515309333801\n",
      "Epoch: 0 Batch: 1090/1186 loss: 0.590732991695404\n",
      "Epoch: 0 Batch: 1091/1186 loss: 0.5997716784477234\n",
      "Epoch: 0 Batch: 1092/1186 loss: 0.5693057775497437\n",
      "Epoch: 0 Batch: 1093/1186 loss: 0.5857285261154175\n",
      "Epoch: 0 Batch: 1094/1186 loss: 0.600658655166626\n",
      "Epoch: 0 Batch: 1095/1186 loss: 0.6902004480361938\n",
      "Epoch: 0 Batch: 1096/1186 loss: 0.7766567468643188\n",
      "Epoch: 0 Batch: 1097/1186 loss: 0.7251132726669312\n",
      "Epoch: 0 Batch: 1098/1186 loss: 0.5872083306312561\n",
      "Epoch: 0 Batch: 1099/1186 loss: 0.5141599774360657\n",
      "Epoch: 0 Batch: 1100/1186 loss: 0.6004074811935425\n",
      "Epoch: 0 Batch: 1101/1186 loss: 0.5803555250167847\n",
      "Epoch: 0 Batch: 1102/1186 loss: 0.4808940887451172\n",
      "Epoch: 0 Batch: 1103/1186 loss: 0.5657747387886047\n",
      "Epoch: 0 Batch: 1104/1186 loss: 0.638810932636261\n",
      "Epoch: 0 Batch: 1105/1186 loss: 0.6763260960578918\n",
      "Epoch: 0 Batch: 1106/1186 loss: 0.5268273949623108\n",
      "Epoch: 0 Batch: 1107/1186 loss: 0.45106127858161926\n",
      "Epoch: 0 Batch: 1108/1186 loss: 0.617987334728241\n",
      "Epoch: 0 Batch: 1109/1186 loss: 0.6620523929595947\n",
      "Epoch: 0 Batch: 1110/1186 loss: 0.5333524942398071\n",
      "Epoch: 0 Batch: 1111/1186 loss: 0.5182815194129944\n",
      "Epoch: 0 Batch: 1112/1186 loss: 0.5987995266914368\n",
      "Epoch: 0 Batch: 1113/1186 loss: 0.5552847385406494\n",
      "Epoch: 0 Batch: 1114/1186 loss: 0.5565969347953796\n",
      "Epoch: 0 Batch: 1115/1186 loss: 0.5496135354042053\n",
      "Epoch: 0 Batch: 1116/1186 loss: 0.6181434392929077\n",
      "Epoch: 0 Batch: 1117/1186 loss: 0.4041752219200134\n",
      "Epoch: 0 Batch: 1118/1186 loss: 0.32222190499305725\n",
      "Epoch: 0 Batch: 1119/1186 loss: 0.4259169399738312\n",
      "Epoch: 0 Batch: 1120/1186 loss: 0.5795257091522217\n",
      "Epoch: 0 Batch: 1121/1186 loss: 0.7614526748657227\n",
      "Epoch: 0 Batch: 1122/1186 loss: 0.6960716247558594\n",
      "Epoch: 0 Batch: 1123/1186 loss: 0.7160524129867554\n",
      "Epoch: 0 Batch: 1124/1186 loss: 0.6531025767326355\n",
      "Epoch: 0 Batch: 1125/1186 loss: 0.45620715618133545\n",
      "Epoch: 0 Batch: 1126/1186 loss: 0.5177816152572632\n",
      "Epoch: 0 Batch: 1127/1186 loss: 0.5627532005310059\n",
      "Epoch: 0 Batch: 1128/1186 loss: 0.5616204738616943\n",
      "Epoch: 0 Batch: 1129/1186 loss: 0.3895013630390167\n",
      "Epoch: 0 Batch: 1130/1186 loss: 0.4377247393131256\n",
      "Epoch: 0 Batch: 1131/1186 loss: 0.4792991280555725\n",
      "Epoch: 0 Batch: 1132/1186 loss: 0.5114776492118835\n",
      "Epoch: 0 Batch: 1133/1186 loss: 0.4965255558490753\n",
      "Epoch: 0 Batch: 1134/1186 loss: 0.7124627828598022\n",
      "Epoch: 0 Batch: 1135/1186 loss: 0.5998747944831848\n",
      "Epoch: 0 Batch: 1136/1186 loss: 0.681437075138092\n",
      "Epoch: 0 Batch: 1137/1186 loss: 0.6405543088912964\n",
      "Epoch: 0 Batch: 1138/1186 loss: 0.6006036400794983\n",
      "Epoch: 0 Batch: 1139/1186 loss: 0.6047477722167969\n",
      "Epoch: 0 Batch: 1140/1186 loss: 0.5561951994895935\n",
      "Epoch: 0 Batch: 1141/1186 loss: 0.6119998693466187\n",
      "Epoch: 0 Batch: 1142/1186 loss: 0.5579094886779785\n",
      "Epoch: 0 Batch: 1143/1186 loss: 0.44636595249176025\n",
      "Epoch: 0 Batch: 1144/1186 loss: 0.8018704056739807\n",
      "Epoch: 0 Batch: 1145/1186 loss: 0.5435211658477783\n",
      "Epoch: 0 Batch: 1146/1186 loss: 0.5255287885665894\n",
      "Epoch: 0 Batch: 1147/1186 loss: 0.5480958819389343\n",
      "Epoch: 0 Batch: 1148/1186 loss: 0.6361585259437561\n",
      "Epoch: 0 Batch: 1149/1186 loss: 0.5717003345489502\n",
      "Epoch: 0 Batch: 1150/1186 loss: 0.4965072572231293\n",
      "Epoch: 0 Batch: 1151/1186 loss: 0.5625177621841431\n",
      "Epoch: 0 Batch: 1152/1186 loss: 0.6331930756568909\n",
      "Epoch: 0 Batch: 1153/1186 loss: 0.6693167686462402\n",
      "Epoch: 0 Batch: 1154/1186 loss: 0.5754639506340027\n",
      "Epoch: 0 Batch: 1155/1186 loss: 0.5051958560943604\n",
      "Epoch: 0 Batch: 1156/1186 loss: 0.5084896087646484\n",
      "Epoch: 0 Batch: 1157/1186 loss: 0.5370962619781494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 1158/1186 loss: 0.6512666940689087\n",
      "Epoch: 0 Batch: 1159/1186 loss: 0.6948625445365906\n",
      "Epoch: 0 Batch: 1160/1186 loss: 0.4584685266017914\n",
      "Epoch: 0 Batch: 1161/1186 loss: 0.5490326285362244\n",
      "Epoch: 0 Batch: 1162/1186 loss: 0.530508279800415\n",
      "Epoch: 0 Batch: 1163/1186 loss: 0.6007548570632935\n",
      "Epoch: 0 Batch: 1164/1186 loss: 0.5453662872314453\n",
      "Epoch: 0 Batch: 1165/1186 loss: 0.5918939709663391\n",
      "Epoch: 0 Batch: 1166/1186 loss: 0.7155564427375793\n",
      "Epoch: 0 Batch: 1167/1186 loss: 0.5886008143424988\n",
      "Epoch: 0 Batch: 1168/1186 loss: 0.6988006234169006\n",
      "Epoch: 0 Batch: 1169/1186 loss: 0.7332509160041809\n",
      "Epoch: 0 Batch: 1170/1186 loss: 0.6783777475357056\n",
      "Epoch: 0 Batch: 1171/1186 loss: 0.5535977482795715\n",
      "Epoch: 0 Batch: 1172/1186 loss: 0.5955678224563599\n",
      "Epoch: 0 Batch: 1173/1186 loss: 0.47223493456840515\n",
      "Epoch: 0 Batch: 1174/1186 loss: 0.48307639360427856\n",
      "Epoch: 0 Batch: 1175/1186 loss: 0.6254425644874573\n",
      "Epoch: 0 Batch: 1176/1186 loss: 0.6041740775108337\n",
      "Epoch: 0 Batch: 1177/1186 loss: 0.7097617983818054\n",
      "Epoch: 0 Batch: 1178/1186 loss: 0.5179080367088318\n",
      "Epoch: 0 Batch: 1179/1186 loss: 0.4658515453338623\n",
      "Epoch: 0 Batch: 1180/1186 loss: 0.48536762595176697\n",
      "Epoch: 0 Batch: 1181/1186 loss: 0.4752521514892578\n",
      "Epoch: 0 Batch: 1182/1186 loss: 0.569769024848938\n",
      "Epoch: 0 Batch: 1183/1186 loss: 0.6264961361885071\n",
      "Epoch: 0 Batch: 1184/1186 loss: 0.5262981057167053\n",
      "Epoch: 0 Batch: 1185/1186 loss: 0.5181697010993958\n",
      "Epoch: 0 Batch: 1186/1186 loss: 0.6062490344047546\n",
      "======== Epoch_end: 0 loss: 0.6062490344047546 time: 293.9938085079193 ========\n"
     ]
    }
   ],
   "source": [
    "# Train neural network\n",
    "\n",
    "EPOCHS = 1\n",
    "start_time = time.time()\n",
    "\n",
    "p_word_to_unknown = 0.02\n",
    "p_char_to_unknown = 0.01\n",
    "\n",
    "\n",
    "\n",
    "dataset = CustomDataset('corpus.train')\n",
    "dataloader = DataLoader(dataset, batch_size = 32)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    batch_n = 0\n",
    "    for X, y in dataloader:\n",
    "        # X - sentence, y - sentence tags\n",
    "        \n",
    "        X = [e.split(\" \") for e in X]\n",
    "        batch_y = [e.split(\" \") for e in y]\n",
    "                \n",
    "        batch_tokens = X.copy()\n",
    "        \n",
    "        max_word_len = -1\n",
    "        max_sentence_len = -1\n",
    "        \n",
    "        \n",
    "        batch_char_X = []\n",
    "        batch_words_X = []\n",
    "        \n",
    "        for sentence in X:\n",
    "            if len(sentence) > max_sentence_len: \n",
    "                max_sentence_len = len(sentence)\n",
    "            for word in sentence:\n",
    "                if len(word) > max_word_len:\n",
    "                    max_word_len = len(word)\n",
    "        \n",
    "        batch_norm_y = []\n",
    "        \n",
    "        for y in batch_y:\n",
    "            y_tags = [tag2idx[e] for e in y]\n",
    "            \n",
    "            while len(y_tags) != max_sentence_len:\n",
    "                y_tags.append(-1)\n",
    "            \n",
    "            \n",
    "            batch_norm_y.append(y_tags)\n",
    "        \n",
    "        batch_norm_y = torch.tensor(batch_norm_y)\n",
    "        \n",
    "        for sentence in X:\n",
    "          \n",
    "            words_ids = []\n",
    "            words_chars_ids = []\n",
    "            \n",
    "            for word in sentence:\n",
    "                \n",
    "                if p_word_to_unknown >= random.uniform(0, 1):\n",
    "                    words_ids.append(word2idx[\"<UNKNOWN>\"])\n",
    "                else:\n",
    "                    words_ids.append(word2idx[word])\n",
    "                char_ids = []\n",
    "                \n",
    "                for c in word:\n",
    "                    if p_char_to_unknown >= random.uniform(0, 1):\n",
    "                        char_ids.append(char2idx['\\r'])\n",
    "                    else:\n",
    "                        char_ids.append(char2idx[c])\n",
    "\n",
    "                words_chars_ids.append(char_ids)\n",
    "            \n",
    "            batch_char_X.append(words_chars_ids)\n",
    "            batch_words_X.append(words_ids)\n",
    "           \n",
    "        my_net.zero_grad()\n",
    "        output = my_net(batch_words_X, batch_char_X, max_word_len, max_sentence_len)\n",
    "        \n",
    "        loss = my_net.loss(output, batch_norm_y)\n",
    "        \n",
    "      \n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        optimezer.step()\n",
    "    \n",
    "    \n",
    "        batch_n+=1\n",
    "        print(f\"Epoch: {epoch} Batch: {batch_n}/{len(dataloader)} loss: {loss}\")          \n",
    "        \n",
    "    print(f\"======== Epoch_end: {epoch} loss: {loss} time: {time.time()-start_time} ========\") \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save((word2idx, tag2idx, idx2tag, char2idx, my_net.state_dict()), 'my_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = open(\"corpus.out\",'w')\n",
    "with open(\"corpus.test\", 'r') as f:\n",
    "    for line in f:\n",
    "        \n",
    "        X = [line.rstrip(\"\\n\").rstrip(\"\\r\").lower().split(\" \")]\n",
    "        tokens = line.rstrip(\"\\n\").rstrip(\"\\r\").split(\" \")\n",
    "        \n",
    "        max_word_len = -1\n",
    "        max_sentence_len = -1\n",
    "        \n",
    "        \n",
    "        batch_char_X = []\n",
    "        batch_words_X = []\n",
    "        \n",
    "        for sentence in X:\n",
    "            if len(sentence) > max_sentence_len: \n",
    "                max_sentence_len = len(sentence)\n",
    "            for word in sentence:\n",
    "                if len(word) > max_word_len:\n",
    "                    max_word_len = len(word)\n",
    "        \n",
    "        \n",
    "        for sentence in X:\n",
    "          \n",
    "            words_ids = []\n",
    "            words_chars_ids = []\n",
    "            \n",
    "            for word in sentence:\n",
    "                \n",
    "                if word not in word2idx:\n",
    "                    words_ids.append(word2idx[\"<UNKNOWN>\"])\n",
    "                else:\n",
    "                    words_ids.append(word2idx[word])\n",
    "                char_ids = []\n",
    "                \n",
    "                for c in word:\n",
    "                    if c not in char2idx:\n",
    "                        char_ids.append(char2idx['\\r'])\n",
    "                    else:\n",
    "                        char_ids.append(char2idx[c])\n",
    "\n",
    "                words_chars_ids.append(char_ids)\n",
    "            \n",
    "            batch_char_X.append(words_chars_ids)\n",
    "            batch_words_X.append(words_ids)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tags = []\n",
    "            output = my_net(batch_words_X, batch_char_X, max_word_len, max_sentence_len)\n",
    "            output = output.view(max_sentence_len,-1)\n",
    "            for e in output:\n",
    "                tags.append(idx2tag[torch.argmax(e).item()])\n",
    "                \n",
    "            \n",
    "        for token, tag in zip(tokens, tags):\n",
    "            out_file.write(token+\"/\"+tag+\" \")\n",
    "        out_file.write('\\n')\n",
    "        \n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.8069869573852978\r\n"
     ]
    }
   ],
   "source": [
    "!python tagger_eval.py corpus.out corpus.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
